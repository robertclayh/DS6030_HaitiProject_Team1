autoplot(rf_tune)
rf_best <- select_best(rf_tune, metric = "rmse")
rf_final <- finalize_workflow(rf_wf, rf_best)
rf_res <- fit_resamples(rf_final, resamples = fish_folds,
metrics = metric_set(rmse, rsq),
control = control_resamples(save_pred = TRUE))
set.seed(1693)
boost_spec <- boost_tree(
mode = "regression",
min_n = tune(),
mtry = tune()
) %>%
set_engine("xgboost")
boost_wf <- workflow() %>%
add_recipe(fish_recipe) %>%
add_model(boost_spec)
boost_params <- extract_parameter_set_dials(boost_wf) %>%
update(
mtry = mtry(range = c(1, ncol(fish_train) - 1)),
min_n = min_n(range = c(2, 40))
)
boost_tune <- tune_grid(
boost_wf,
resamples = fish_folds,
grid = grid_regular(boost_params, levels = 40),
metrics = metric_set(rmse, rsq)
)
autoplot(boost_tune)
boost_best <- select_best(boost_tune, metric = "rmse")
boost_final <- finalize_workflow(boost_wf, boost_best)
boost_res <- fit_resamples(boost_final, resamples = fish_folds,
metrics = metric_set(rmse, rsq),
control = control_resamples(save_pred = TRUE))
knn_spec <- nearest_neighbor(
mode = "regression",
neighbors = tune()
) %>%
set_engine("kknn")
knn_wf <- workflow() %>%
add_recipe(fish_recipe) %>%
add_model(knn_spec)
knn_params <- extract_parameter_set_dials(knn_wf) %>%
update(neighbors = neighbors(range = c(1, 15)))
knn_tune <- tune_grid(
knn_wf,
resamples = fish_folds,
grid = grid_regular(knn_params, levels = 15),
metrics = metric_set(rmse, rsq)
)
autoplot(knn_tune)
knn_best <- select_best(knn_tune, metric = "rmse")
knn_final <- finalize_workflow(knn_wf, knn_best)
knn_res <- fit_resamples(knn_final, resamples = fish_folds,
metrics = metric_set(rmse, rsq),
control = control_resamples(save_pred = TRUE))
all_metrics <- bind_rows(
collect_metrics(lm_res) %>% mutate(Model = "Linear Regression"),
collect_metrics(rf_res) %>% mutate(Model = "Random Forest"),
collect_metrics(boost_res) %>% mutate(Model = "Boosting"),
collect_metrics(knn_res) %>% mutate(Model = "KNN")
)
summary_metrics <- all_metrics %>%
filter(.metric %in% c("rmse", "rsq")) %>%
select(Model, .metric, mean) %>%
pivot_wider(names_from = .metric, values_from = mean)
kable(summary_metrics, digits = 4, caption = "Mean RMSE and Mean r^2 for All Models") %>%
kable_styling(full_width = FALSE)
final_lm_model <- lm_wf %>%
fit(data = fish_train)
final_rf_model <- finalize_workflow(rf_wf, rf_best) %>%
fit(data = fish_train)
final_boost_model <- finalize_workflow(boost_wf, boost_best) %>%
fit(data = fish_train)
final_knn_model <- finalize_workflow(knn_wf, knn_best) %>%
fit(data = fish_train)
lm_test_preds    <- predict(final_lm_model, new_data = fish_test) %>% bind_cols(fish_test)
rf_test_preds    <- predict(final_rf_model, new_data = fish_test) %>% bind_cols(fish_test)
boost_test_preds <- predict(final_boost_model, new_data = fish_test) %>% bind_cols(fish_test)
knn_test_preds   <- predict(final_knn_model, new_data = fish_test) %>% bind_cols(fish_test)
lm_metrics <- lm_test_preds %>%
metrics(truth = LC50, estimate = .pred) %>%
filter(.metric %in% c("rmse", "mae")) %>%
mutate(Model = "Linear Regression")
rf_metrics <- rf_test_preds %>%
metrics(truth = LC50, estimate = .pred) %>%
filter(.metric %in% c("rmse", "mae")) %>%
mutate(Model = "Random Forest")
boost_metrics <- boost_test_preds %>%
metrics(truth = LC50, estimate = .pred) %>%
filter(.metric %in% c("rmse", "mae")) %>%
mutate(Model = "Boosting")
knn_metrics <- knn_test_preds %>%
metrics(truth = LC50, estimate = .pred) %>%
filter(.metric %in% c("rmse", "mae")) %>%
mutate(Model = "KNN")
all_test_metrics <- bind_rows(lm_metrics, rf_metrics, boost_metrics, knn_metrics) %>%
select(Model, .metric, .estimate) %>%
pivot_wider(names_from = .metric, values_from = .estimate)
kable(all_test_metrics, digits = 4,
caption = "Test Set Performance Metrics (RMSE and MAE)") %>%
kable_styling(full_width = FALSE)
train_rmse_tbl <- summary_metrics %>%
select(Model, rmse) %>%
mutate(Data = "Training")
test_rmse_tbl <- all_test_metrics %>%
select(Model, rmse) %>%
mutate(Data = "Test")
combined_rmse_tbl <- bind_rows(train_rmse_tbl, test_rmse_tbl)
wide_rmse_tbl <- combined_rmse_tbl %>%
pivot_wider(names_from = Data, values_from = rmse) %>%
mutate(diff = Test - Training)
rmse_plot <- ggplot(wide_rmse_tbl, aes(y = Model)) +
geom_point(aes(x = Training, color = "Training"), size = 3) +
geom_point(aes(x = Test, color = "Test"), size = 3) +
geom_segment(aes(x = Training, xend = Test, yend = Model), size = 1, color = "gray") +
geom_text(aes(x = (Training + Test) / 2, label = paste("Diff =", round(diff, 3))),
vjust = -0.8, size = 3.5) +
labs(title = "RMSE for Training and Test Sets Across Models",
x = "RMSE", y = "Model", color = "Data") +
theme_minimal()
print(rmse_plot)
lm_preds <- collect_predictions(lm_res) %>% mutate(residual = LC50 - .pred)
rf_preds <- collect_predictions(rf_res) %>% mutate(residual = LC50 - .pred)
boost_preds <- collect_predictions(boost_res) %>% mutate(residual = LC50 - .pred)
knn_preds <- collect_predictions(knn_res) %>% mutate(residual = LC50 - .pred)
# Linear Regression
p_lm <- ggplot(lm_preds, aes(x = .pred, y = residual)) +
geom_point(alpha = 0.3) +
geom_smooth(method = "loess", color = "blue") +
labs(title = "Linear Regression", x = "Predicted LC50", y = "Residuals") +
theme_minimal()
# Random Forest
p_rf <- ggplot(rf_preds, aes(x = .pred, y = residual)) +
geom_point(alpha = 0.3) +
geom_smooth(method = "loess", color = "blue") +
labs(title = "Random Forest", x = "Predicted LC50", y = "Residuals") +
theme_minimal()
# Boosting
p_boost <- ggplot(boost_preds, aes(x = .pred, y = residual)) +
geom_point(alpha = 0.3) +
geom_smooth(method = "loess", color = "blue") +
labs(title = "Boosting", x = "Predicted LC50", y = "Residuals") +
theme_minimal()
# KNN
p_knn <- ggplot(knn_preds, aes(x = .pred, y = residual)) +
geom_point(alpha = 0.3) +
geom_smooth(method = "loess", color = "blue") +
labs(title = "KNN", x = "Predicted LC50", y = "Residuals") +
theme_minimal()
combined_plot <- (p_lm + p_rf) / (p_boost + p_knn)
print(combined_plot)
final_boost_model %>%
extract_fit_parsnip() %>%
vip(num_features = 10)
stopCluster(cl)
registerDoSEQ()
setwd("~/Documents/MSDS/DS6030/M08")
all_metrics <- bind_rows(
collect_metrics(lm_res) %>% mutate(Model = "Linear Regression"),
collect_metrics(rf_res) %>% mutate(Model = "Random Forest"),
collect_metrics(boost_res) %>% mutate(Model = "Boosting"),
collect_metrics(knn_res) %>% mutate(Model = "KNN")
)
summary_metrics <- all_metrics %>%
filter(.metric %in% c("rmse", "rsq")) %>%
select(Model, .metric, mean) %>%
pivot_wider(names_from = .metric, values_from = mean)
kable(summary_metrics, digits = 4, caption = "Mean RMSE and Mean $r^2$ for All Models") %>%
kable_styling(full_width = FALSE)
setwd("~/Documents/MSDS/DS6030/DS6030_HaitiProject_Team1")
knitr::opts_chunk$set(
echo = FALSE,
cache = TRUE,
autodep = TRUE,
fig.align = "center",
fig.pos = "H",
out.width = "100%"
)
# Set eval to TRUE if you want to see the R code outputs
knitr::opts_chunk$set(
echo = TRUE)
#| cache: FALSE
#| message: FALSE
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
#| cache: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(tidymodels)
library(discrim)
library(leaflet)
library(terra)
library(htmlwidgets)
library(leafem)
library(colordistance)
library(jpeg)
library(patchwork)
library(probably)
library(gridExtra)
library(plotly)
library(mapview)
library(farver)
library(kableExtra)
library(leaflet.extras2)
library(webshot2)
#| message: FALSE
#| warning: FALSE
col_names <- c('ID','X','Y','Map X','Map Y','Lat','Lon','Red','Green','Blue')
blue_files <- c(
"orthovnir069_ROI_Blue_Tarps.txt",
"orthovnir067_ROI_Blue_Tarps.txt",
"orthovnir078_ROI_Blue_Tarps.txt"
)
non_blue_files <- c(
"orthovnir057_ROI_NON_Blue_Tarps.txt",
"orthovnir078_ROI_NON_Blue_Tarps.txt",
"orthovnir067_ROI_NOT_Blue_Tarps.txt",
"orthovnir069_ROI_NOT_Blue_Tarps.txt"
)
blue_data <- map_dfr(blue_files, ~
read_table(.x, comment = ";", col_names = col_names, col_types = cols(
`Map X` = col_double(),
`Map Y` = col_double(),
Red = col_integer(),
Green = col_integer(),
Blue = col_integer()
)) %>%
select(`Map X`, `Map Y`, Red, Green, Blue) %>%
mutate(BT = "TRUE")
)
non_blue_data <- map_dfr(non_blue_files, ~
read_table(.x, comment = ";", col_names = col_names, col_types = cols(
`Map X` = col_double(),
`Map Y` = col_double(),
Red = col_integer(),
Green = col_integer(),
Blue = col_integer()
)) %>%
select(`Map X`, `Map Y`, Red, Green, Blue) %>%
mutate(BT = "FALSE")
)
holdout_data <- bind_rows(blue_data, non_blue_data) %>%
mutate(BT = factor(BT, levels = c("TRUE", "FALSE")))
#| message: FALSE
train_data <- read_csv("HaitiPixels.csv") %>%
mutate(BT = factor(if_else(Class == "Blue Tarp", "TRUE", "FALSE"), levels = c("TRUE", "FALSE"))) %>%
select(Red, Green, Blue, BT)
convert_color_spaces <- function(data) {
# Convert RGB to CIELab
lab_values <- farver::convert_colour(
as.matrix(data[, c("Red", "Green", "Blue")]),
from = "rgb",
to = "lab"
)
# Convert RGB to HSV
hsv_values <- farver::convert_colour(
as.matrix(data[, c("Red", "Green", "Blue")]),
from = "rgb",
to = "hsv"
)
# Convert
lab_df <- as.data.frame(lab_values)
colnames(lab_df) <- c("Luminance", "a", "b")
hsv_df <- as.data.frame(hsv_values)
colnames(hsv_df) <- c("Hue", "Saturation", "Value")
# Bind new columns
data <- cbind(data, lab_df, hsv_df)
return(data)
}
# Apply function
train_data <- convert_color_spaces(train_data)
holdout_data <- convert_color_spaces(holdout_data)
knitr::opts_chunk$set(
echo = FALSE,
cache = TRUE,
autodep = TRUE,
fig.align = "center",
fig.pos = "H",
out.width = "100%"
)
#| cache: FALSE
#| message: FALSE
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
#| cache: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(tidymodels)
library(discrim)
library(leaflet)
library(terra)
library(htmlwidgets)
library(leafem)
library(colordistance)
library(jpeg)
library(patchwork)
library(probably)
library(gridExtra)
library(plotly)
library(mapview)
library(farver)
library(kableExtra)
library(leaflet.extras2)
library(webshot2)
#| message: FALSE
#| warning: FALSE
col_names <- c('ID','X','Y','Map X','Map Y','Lat','Lon','Red','Green','Blue')
blue_files <- c(
"orthovnir069_ROI_Blue_Tarps.txt",
"orthovnir067_ROI_Blue_Tarps.txt",
"orthovnir078_ROI_Blue_Tarps.txt"
)
non_blue_files <- c(
"orthovnir057_ROI_NON_Blue_Tarps.txt",
"orthovnir078_ROI_NON_Blue_Tarps.txt",
"orthovnir067_ROI_NOT_Blue_Tarps.txt",
"orthovnir069_ROI_NOT_Blue_Tarps.txt"
)
blue_data <- map_dfr(blue_files, ~
read_table(.x, comment = ";", col_names = col_names, col_types = cols(
`Map X` = col_double(),
`Map Y` = col_double(),
Red = col_integer(),
Green = col_integer(),
Blue = col_integer()
)) %>%
select(`Map X`, `Map Y`, Red, Green, Blue) %>%
mutate(BT = "TRUE")
)
non_blue_data <- map_dfr(non_blue_files, ~
read_table(.x, comment = ";", col_names = col_names, col_types = cols(
`Map X` = col_double(),
`Map Y` = col_double(),
Red = col_integer(),
Green = col_integer(),
Blue = col_integer()
)) %>%
select(`Map X`, `Map Y`, Red, Green, Blue) %>%
mutate(BT = "FALSE")
)
holdout_data <- bind_rows(blue_data, non_blue_data) %>%
mutate(BT = factor(BT, levels = c("TRUE", "FALSE")))
#| message: FALSE
train_data <- read_csv("HaitiPixels.csv") %>%
mutate(BT = factor(if_else(Class == "Blue Tarp", "TRUE", "FALSE"), levels = c("TRUE", "FALSE"))) %>%
select(Red, Green, Blue, BT)
convert_color_spaces <- function(data) {
# Convert RGB to CIELab
lab_values <- farver::convert_colour(
as.matrix(data[, c("Red", "Green", "Blue")]),
from = "rgb",
to = "lab"
)
# Convert RGB to HSV
hsv_values <- farver::convert_colour(
as.matrix(data[, c("Red", "Green", "Blue")]),
from = "rgb",
to = "hsv"
)
# Convert
lab_df <- as.data.frame(lab_values)
colnames(lab_df) <- c("Luminance", "a", "b")
hsv_df <- as.data.frame(hsv_values)
colnames(hsv_df) <- c("Hue", "Saturation", "Value")
# Bind new columns
data <- cbind(data, lab_df, hsv_df)
return(data)
}
# Apply function
train_data <- convert_color_spaces(train_data)
holdout_data <- convert_color_spaces(holdout_data)
View(train_data)
formula <- BT ~ Red + Green + Blue
fish_recipe <- recipe(LC50 ~ ., data = train_data)
formula <- BT ~ Red + Green + Blue
fish_recipe <- recipe(formula, data = train_data)
knn_spec <- nearest_neighbor(
mode = "regression",
neighbors = tune()
) %>%
set_engine("kknn")
knn_wf <- workflow() %>%
add_recipe(fish_recipe) %>%
add_model(knn_spec)
knn_params <- extract_parameter_set_dials(knn_wf) %>%
update(neighbors = neighbors(range = c(1, 15)))
knn_tune <- tune_grid(
knn_wf,
resamples = fish_folds,
grid = grid_regular(knn_params, levels = 15),
metrics = metric_set(rmse, rsq)
)
# RGB Model Formula and Recipe
rgb_formula <- BT ~ Red + Green + Blue
rgb_recipe <- recipe(rgb_formula, data = train_data)
# Define cross-validation approach
set.seed(6030)
resamples <- vfold_cv(train_data, v = 10, strata = BT)
custom_metrics <- metric_set(roc_auc, accuracy, precision, f_meas)
cv_control <- control_resamples(save_pred = TRUE)
knn_spec <- nearest_neighbor(
mode = "classification",
neighbors = tune()
) %>%
set_engine("kknn")
knn_wf <- workflow() %>%
add_recipe(fish_recipe) %>%
add_model(knn_spec)
knn_params <- extract_parameter_set_dials(knn_wf) %>%
update(neighbors = neighbors(range = c(1, 15)))
knn_tune <- tune_grid(
knn_wf,
resamples = resamples,
grid = grid_regular(knn_params, levels = 15),
metrics = metric_set(rmse, rsq)
)
# RGB Model Formula and Recipe
rgb_formula <- BT ~ Red + Green + Blue
rgb_recipe <- recipe(rgb_formula, data = train_data)
# Define cross-validation approach
set.seed(6030)
resamples <- vfold_cv(train_data, v = 10, strata = BT)
custom_metrics <- metric_set(roc_auc, accuracy, precision, f_meas)
cv_control <- control_resamples(save_pred = TRUE)
knn_spec <- nearest_neighbor(
mode = "classification",
neighbors = tune()
) %>%
set_engine("kknn")
knn_wf <- workflow() %>%
add_recipe(fish_recipe) %>%
add_model(knn_spec)
knn_params <- extract_parameter_set_dials(knn_wf) %>%
update(neighbors = neighbors(range = c(1, 15)))
knn_tune <- tune_grid(
knn_wf,
resamples = resamples,
grid = grid_regular(knn_params, levels = 15),
metrics = custom_metrics
)
# Define RGB model formula and recipe
rgb_formula <- BT ~ Red + Green + Blue
rgb_recipe <- recipe(rgb_formula, data = train_data)
# Set up 10-fold cross-validation (stratified by BT)
set.seed(6030)
resamples <- vfold_cv(train_data, v = 10, strata = BT)
custom_metrics <- metric_set(roc_auc, accuracy, precision, f_meas)
cv_control <- control_resamples(save_pred = TRUE)
# Define the k-NN specification with tunable neighbors
knn_spec <- nearest_neighbor(
mode = "classification",
neighbors = tune()
) %>%
set_engine("kknn")
# Create workflow
knn_wf <- workflow() %>%
add_recipe(rgb_recipe) %>%
add_model(knn_spec)
# Set tuning grid for neighbors (range: 1 to 15)
knn_params <- extract_parameter_set_dials(knn_wf) %>%
update(neighbors = neighbors(range = c(1, 15)))
knn_tune <- tune_grid(
knn_wf,
resamples = resamples,
grid = grid_regular(knn_params, levels = 15),
metrics = custom_metrics
)
# Define RGB model formula and recipe
rgb_formula <- BT ~ Red + Green + Blue
rgb_recipe <- recipe(rgb_formula, data = train_data)
# Set up 10-fold cross-validation (stratified by BT)
set.seed(6030)
resamples <- vfold_cv(train_data, v = 10, strata = BT)
custom_metrics <- metric_set(roc_auc, accuracy, precision, f_meas)
cv_control <- control_resamples(save_pred = TRUE)
# Define the k-NN specification with tunable neighbors
knn_spec <- nearest_neighbor(
mode = "classification",
neighbors = tune()
) %>%
set_engine("kknn")
# Create workflow
knn_wf <- workflow() %>%
add_recipe(rgb_recipe) %>%
add_model(knn_spec)
# Define the k-NN specification with tunable neighbors
knn_spec <- nearest_neighbor(
mode = "classification",
neighbors = tune()
) %>%
set_engine("kknn")
# Create workflow with the RGB recipe
knn_wf <- workflow() %>%
add_recipe(rgb_recipe) %>%
add_model(knn_spec)
# Extract the parameter set and update the neighbor range
knn_params <- parameters(knn_wf) %>%
update(neighbors = neighbors(range = c(1, 15)))
# Create a regular tuning grid with 15 levels for the neighbors parameter
grid <- grid_regular(knn_params, levels = 15)
# Tune the grid using 10-fold cross-validation
knn_tune <- tune_grid(
knn_wf,
resamples = resamples,
grid = grid,
metrics = custom_metrics,
control = cv_control
)
