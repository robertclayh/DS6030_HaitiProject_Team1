---
title: "Project Part 1"
author: "Virginia Brame, Clay Harris, Hai Liu"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  word_document: default
header-includes: \usepackage{float}
---
```{r setup}
knitr::opts_chunk$set(
  echo = FALSE,
  cache = TRUE,
  autodep = TRUE,
  fig.align = "center",
  fig.pos = "H",
  out.width = "100%"
)
```

```{r echo true, eval=FALSE}
# Set eval to TRUE if you want to see the R code outputs
knitr::opts_chunk$set(
  echo = TRUE)
```

## Data (loading, wrangling, EDA)

```{r parallel}
#| cache: FALSE
#| message: FALSE
library(future)
plan(multisession, workers = 11)
```

```{r libraries}
#| cache: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(tidymodels)
library(discrim)
library(leaflet)
library(terra)
library(htmlwidgets)
library(leafem)
library(colordistance)
library(jpeg)
library(patchwork)
library(probably)
library(gridExtra)
library(plotly)
library(mapview)
library(farver)
library(kableExtra)
library(leaflet.extras2)
library(webshot2)
```

### Data loading and wrangling

```{r holdout data processing}
#| message: FALSE
#| warning: FALSE

col_names <- c('ID','X','Y','Map X','Map Y','Lat','Lon','Red','Green','Blue')

blue_files <- c(
  "orthovnir069_ROI_Blue_Tarps.txt",
  "orthovnir067_ROI_Blue_Tarps.txt",
  "orthovnir078_ROI_Blue_Tarps.txt"
)

non_blue_files <- c(
  "orthovnir057_ROI_NON_Blue_Tarps.txt",
  "orthovnir078_ROI_NON_Blue_Tarps.txt",
  "orthovnir067_ROI_NOT_Blue_Tarps.txt",
  "orthovnir069_ROI_NOT_Blue_Tarps.txt"
)

blue_data <- map_dfr(blue_files, ~ 
  read_table(.x, comment = ";", col_names = col_names, col_types = cols(
    `Map X` = col_double(),
    `Map Y` = col_double(),
    Red = col_integer(),
    Green = col_integer(),
    Blue = col_integer()
  )) %>% 
    select(`Map X`, `Map Y`, Red, Green, Blue) %>% 
    mutate(BT = "TRUE")
)

non_blue_data <- map_dfr(non_blue_files, ~ 
  read_table(.x, comment = ";", col_names = col_names, col_types = cols(
    `Map X` = col_double(),
    `Map Y` = col_double(),
    Red = col_integer(),
    Green = col_integer(),
    Blue = col_integer()
  )) %>% 
    select(`Map X`, `Map Y`, Red, Green, Blue) %>% 
    mutate(BT = "FALSE")
)

holdout_data <- bind_rows(blue_data, non_blue_data) %>% 
  mutate(BT = factor(BT, levels = c("TRUE", "FALSE")))
```

```{r training data processing}
#| message: FALSE

train_data <- read_csv("HaitiPixels.csv") %>%
  mutate(BT = factor(if_else(Class == "Blue Tarp", "TRUE", "FALSE"), levels = c("TRUE", "FALSE"))) %>%
  select(Red, Green, Blue, BT)
```

```{r add CIELab and HSV}
convert_color_spaces <- function(data) {
  # Convert RGB to CIELab
  lab_values <- farver::convert_colour(
    as.matrix(data[, c("Red", "Green", "Blue")]), 
    from = "rgb", 
    to = "lab"
  )
  
  # Convert RGB to HSV
  hsv_values <- farver::convert_colour(
    as.matrix(data[, c("Red", "Green", "Blue")]), 
    from = "rgb", 
    to = "hsv"
  )
  
  # Convert
  lab_df <- as.data.frame(lab_values)
  colnames(lab_df) <- c("Luminance", "a", "b") 
  hsv_df <- as.data.frame(hsv_values)
  colnames(hsv_df) <- c("Hue", "Saturation", "Value")
  
  # Bind new columns
  data <- cbind(data, lab_df, hsv_df)
  
  return(data)
}

# Apply function
train_data <- convert_color_spaces(train_data)
holdout_data <- convert_color_spaces(holdout_data)
```

```{r calculate prop values}
# For the training data
train_data <- train_data %>%
  mutate(total = Red + Green + Blue,
         Red_Prop = Red / total,
         Green_Prop = Green / total,
         Blue_Prop = Blue / total) %>%
  select(-total)

# For the holdout data
holdout_data <- holdout_data %>%
  mutate(total = Red + Green + Blue,
         Red_Prop = Red / total,
         Green_Prop = Green / total,
         Blue_Prop = Blue / total) %>%
  select(-total)
```

```{r calculate dispersion}
# For the training data
train_data <- train_data %>%
  mutate(
    Dispersion = abs(Red_Prop - 1/3) + abs(Green_Prop - 1/3) + abs(Blue_Prop - 1/3)
  )

# For the holdout data
holdout_data <- holdout_data %>%
  mutate(
    Dispersion = abs(Red_Prop - 1/3) + abs(Green_Prop - 1/3) + abs(Blue_Prop - 1/3)
  )
```

```{r calculate shifted hue}
# For the training data
train_data <- train_data %>%
  mutate(Hue_Shifted = (Hue + (360 * 0.25)) %% 1)

# For the holdout data
holdout_data <- holdout_data %>%
  mutate(Hue_Shifted = (Hue + (360 * 0.25)) %% 1)
```

```{r lasso}
formula <- BT ~ Red + Green + Blue + Luminance + a + b + Hue + Saturation + Value + Red_Prop + Green_Prop + Blue_Prop +Dispersion + Hue_Shifted

rec <- recipe(formula, data = train_data) %>%
  step_normalize(all_numeric_predictors())

set.seed(1) # for reproducibility
resamples <- vfold_cv(train_data, v=10, strata=BT)
metrics <- metric_set(roc_auc, accuracy)
cv_control <- control_resamples(save_pred=TRUE)

tune_logreg_spec <- logistic_reg(engine="glmnet", mode="classification",
                                 penalty=tune(), mixture=tune())

tune_logreg_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(tune_logreg_spec)
logreg_params <- extract_parameter_set_dials(tune_logreg_wf) %>%
  update(
    penalty=penalty(c(-3, -0.5)),
    mixture=mixture(c(0, 1)))

tune_results_logreg <- tune_grid(tune_logreg_wf,
                                 resamples=resamples,
                                 control=cv_control,
                                 metrics = metric_set(roc_auc, accuracy),
                                 grid=grid_random(logreg_params, size=50))

autoplot(tune_results_logreg)
```

```{r select best}

# Step 1: Extract best parameters from the tuning results object
best_logreg <- select_best(tune_results_logreg, metric = "roc_auc")

# Step 2: Finalize the workflow with the best parameters
final_logreg_fit <- finalize_workflow(tune_logreg_wf, best_logreg) %>% 
  fit(data = train_data)

# Step 3: Extract the model coefficients
# (for a logistic regression model, this will show the coefficients from the fitted glm model)
tidy(final_logreg_fit)
```

```{r cross validation approach}
# Define cross-validation approach
set.seed(1)

resamples <- vfold_cv(train_data, v = 10, strata = BT)
custom_metrics <- metric_set(roc_auc, accuracy, precision, f_meas)
cv_control <- control_resamples(save_pred = TRUE)
```

Cross-validation
```{r cross validate}
#| message: FALSE
#| warning: FALSE

# Finalize the workflow using the best parameters
final_logreg_wf <- finalize_workflow(tune_logreg_wf, best_logreg)

# Generate a cross-validation object from the finalized workflow
final_logreg_cv <- fit_resamples(
  final_logreg_wf,
  resamples = resamples,
  metrics = custom_metrics,
  control = cv_control
)

# Cross-validation for RGB models
logreg_rgb_cv <- fit_resamples(logreg_rgb_wf, resamples, metrics = custom_metrics, control = cv_control)

# Cross-validation for CIELab models
logreg_lab_cv <- fit_resamples(logreg_lab_wf, resamples, metrics = custom_metrics, control = cv_control)

# Cross-validation for HSV models
logreg_hsv_cv <- fit_resamples(logreg_hsv_wf, resamples, metrics = custom_metrics, control = cv_control)
```

```{r set formulas}
# RGB Model Formula and Recipe
rgb_formula <- BT ~ Red + Green + Blue
rgb_recipe <- recipe(rgb_formula, data = train_data)

# CIELab Model Formula and Recipe
lab_formula <- BT ~ Luminance + a + b
lab_recipe <- recipe(lab_formula, data = train_data)

# HSV Model Formula and Recipe
hsv_formula <- BT ~ Hue + Saturation + Value
hsv_recipe <- recipe(hsv_formula, data = train_data)
```

```{r specify models}
# Specify models
logreg_spec <- logistic_reg(mode="classification", engine="glm")
```

```{r define workflows}
# RGB Models
logreg_rgb_wf <- workflow() %>% add_recipe(rgb_recipe) %>% add_model(logreg_spec)

# CIELab Models
logreg_lab_wf <- workflow() %>% add_recipe(lab_recipe) %>% add_model(logreg_spec)

# HSV Models
logreg_hsv_wf <- workflow() %>% add_recipe(hsv_recipe) %>% add_model(logreg_spec)
```

```{r fit final models on train}
#| message: FALSE
#| warning: FALSE

# Fit final models on train_data for RGB
final_logreg_rgb_fit <- logreg_rgb_wf %>% fit(data = train_data)

# Fit final models on train_data for CIELab
final_logreg_lab_fit <- logreg_lab_wf %>% fit(data = train_data)

# Fit final models on train_data for HSV
final_logreg_hsv_fit <- logreg_hsv_wf %>% fit(data = train_data)
```

```{r autplot roc}

# Obtain predictions for each model on training data
pred_logreg     <- augment(final_logreg_fit, new_data = train_data)
pred_logreg_rgb <- augment(final_logreg_rgb_fit, new_data = train_data)
pred_logreg_lab <- augment(final_logreg_lab_fit, new_data = train_data)
pred_logreg_hsv <- augment(final_logreg_hsv_fit, new_data = train_data)

# Compute ROC curves for each model (using .pred_TRUE as the estimated probability)
roc_logreg     <- roc_curve(pred_logreg, truth = BT, .pred_TRUE, event_level = "first") %>% mutate(model = "Logistic Regression")
roc_logreg_rgb <- roc_curve(pred_logreg_rgb, truth = BT, .pred_TRUE, event_level = "first") %>% mutate(model = "Logistic Regression (RGB)")
roc_logreg_lab <- roc_curve(pred_logreg_lab, truth = BT, .pred_TRUE, event_level = "first") %>% mutate(model = "Logistic Regression (CIELab)")
roc_logreg_hsv <- roc_curve(pred_logreg_hsv, truth = BT, .pred_TRUE, event_level = "first") %>% mutate(model = "Logistic Regression (HSV)")

# Combine all ROC curves
roc_all <- bind_rows(roc_logreg, roc_logreg_rgb, roc_logreg_lab, roc_logreg_hsv)

# Option 2: Using yardstick's autoplot (if it supports the grouping variable)
autoplot(roc_all) +
  labs(title = "Overlay of ROC Curves for Logistic Regression Models",
       x = "1 - Specificity",
       y = "Sensitivity",
       color = "Model")
```

```{r threshold graphs 1}
threshold_graph <- function(model_cv, model_name) {
    performance <- probably::threshold_perf(collect_predictions(model_cv), BT, .pred_TRUE,
        thresholds=seq(0.01, 0.99, 0.01), event_level="first",
        metrics=metric_set(f_meas, accuracy, sens))
    max_metrics <- performance %>%
        drop_na() %>%
        group_by(.metric) %>%
        filter(.estimate == max(.estimate))
    g <- ggplot(performance, aes(x=.threshold, y=.estimate, color=.metric)) +
        geom_line() +
        geom_point(data=max_metrics, color="black") +
        labs(title=model_name, x="Threshold", y="Metric value") +
        coord_cartesian(ylim=c(0, 1))
    thresholds <- max_metrics %>%
        select(.metric, .threshold) %>%
        deframe()
    return(list(graph=g, thresholds=thresholds))
}

visualize_conf_mat <- function(model_cv, thresholds, metric) {
    threshold <- thresholds[metric]
    cm <- collect_predictions(model_cv) %>%
        mutate(
            .pred_class = make_two_class_pred(.pred_TRUE, c("TRUE", "FALSE"), threshold=threshold),
        ) %>%
        conf_mat(truth=BT, estimate=.pred_class)
    autoplot(cm, type="heatmap") +
        labs(title=sprintf("Threshold %.2f (%s)", threshold, metric))
}

overview_model <- function(model_cv, model_name) {
    tg <- threshold_graph(model_cv, model_name)
    g1 <- visualize_conf_mat(model_cv, tg$thresholds, "accuracy")
    g2 <- visualize_conf_mat(model_cv, tg$thresholds, "f_meas")
    g3 <- visualize_conf_mat(model_cv, tg$thresholds, "sens")
    tg$graph + (g1 / g2 / g3)
}
```

```{r threshold measure apply function}
# RGB Models
rgb_g1 <- overview_model(logreg_rgb_cv, "Logistic Regression (RGB)")

# CIELab Models
lab_g1 <- overview_model(logreg_lab_cv, "Logistic Regression (CIELab)")

# HSV Models
hsv_g1 <- overview_model(logreg_hsv_cv, "Logistic Regression (HSV)")

#Tuned Model
tuned_g1 <- overview_model(final_logreg_cv, "Tuned Logistic Regression (Elastic Net)")
```

```{r print first threshold imgs}
#| fig.width: 10
#| fig.height: 15
#| out.width: 100%
#| fig.cap: Metrics as a function of threshold optimization across all color spaces.
#| warning: FALSE
# Arrange in 3 rows (RGB, CIELab, HSV)
combined_threshold_plots <- (rgb_g1) /
                            (lab_g1) /
                            (hsv_g1) /
                            (tuned_g1)

# Print the combined plot
combined_threshold_plots

ggsave("combined_threshold_plots_elastic.png", plot = combined_threshold_plots, width = 10, height = 15, dpi = 600)
```

```{r compare roc-auc cv and full train}
compute_roc_diff <- function(cv_object, workflow, train_data) {
  # Cross-validation ROC-AUC (mean value)
  cv_roc <- collect_metrics(cv_object) %>%
    filter(.metric == "roc_auc") %>%
    pull(mean)
  
  # Fit the model
  final_fit <- workflow %>% fit(train_data)
  
  # Evaluate ROC-AUC
  full_preds <- augment(final_fit, new_data = train_data)
  
  full_roc <- roc_auc(full_preds, truth = BT, .pred_TRUE, event_level = "first") %>%
    pull(.estimate)
  
  # Return a tibble with ROC values and their difference
  tibble(
    cv_roc = cv_roc,
    full_roc = full_roc,
    diff = full_roc - cv_roc
  )
}

roc_diff_results <- bind_rows(
  compute_roc_diff(logreg_rgb_cv, logreg_rgb_wf, train_data) %>% 
    mutate(model = "Logistic Regression", color_space = "RGB"),
  
  compute_roc_diff(logreg_lab_cv, logreg_lab_wf, train_data) %>% 
    mutate(model = "Logistic Regression", color_space = "CIELab"),
  
  compute_roc_diff(logreg_hsv_cv, logreg_hsv_wf, train_data) %>% 
    mutate(model = "Logistic Regression", color_space = "HSV"),
  
   compute_roc_diff(final_logreg_cv, final_logreg_wf, train_data) %>% 
    mutate(model = "Tuned Logistic Regression (Elastic Net)", color_space = "All Predictor Variables")
)

roc_diff_results <- roc_diff_results %>%
  mutate(
    color_space = factor(color_space, levels = c("RGB", "CIELab", "HSV", "All Predictor Variables")),
    model = factor(model, levels = c("Logistic Regression", "Tuned Logistic Regression (Elastic Net)"))
  ) %>%
  arrange(color_space, model) %>%
  select(color_space, model, cv_roc, full_roc, diff) %>%
  kable(
    caption = "Comparison of ROC-AUC between cross-validation and full-training fits",
    digits = 6,
    col.names = c("Color Space", "Model", "ROC-AUC of CV Folds", "ROC-AUC of Fitted Model", "Difference")
  ) %>%
  kable_styling(full_width = FALSE) %>%
  collapse_rows(columns = 1, valign = "top")

roc_diff_results
```

```{r save roc cv kable}
save_kable(roc_diff_results, file = "roc_diff_results_elastic.png", zoom = 2)
```
