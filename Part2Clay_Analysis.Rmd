---
title: "Project Part 1"
author: "Virginia Brame, Clay Harris, Hai Liu"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  word_document: default
header-includes: \usepackage{float}
---
```{r setup}
knitr::opts_chunk$set(
  echo = FALSE,
  cache = TRUE,
  autodep = TRUE,
  fig.align = "center",
  fig.pos = "H",
  out.width = "100%"
)
```

```{r echo true, eval=FALSE}
# Set eval to TRUE if you want to see the R code outputs
knitr::opts_chunk$set(
  echo = TRUE)
```

## Data (loading, wrangling, EDA)

```{r parallel}
#| cache: FALSE
#| message: FALSE
library(future)
plan(multisession, workers = 23)
```

```{r libraries}
#| cache: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(tidymodels)
library(discrim)
library(leaflet)
library(terra)
library(htmlwidgets)
library(leafem)
library(colordistance)
library(jpeg)
library(patchwork)
library(probably)
library(gridExtra)
library(plotly)
library(mapview)
library(farver)
library(kableExtra)
library(leaflet.extras2)
library(webshot2)
```

```{r lasso}
formula <- BT ~ Red + Green + Blue + Luminance + a + b + Hue + Saturation + Value + Red_Prop + Green_Prop + Blue_Prop + Dispersion + Hue_Shifted + Red_9 + Green_9 + Blue_9 + Luminance_9 + a_9 + b_9 + Hue_9 + Saturation_9 + Value_9 + Red_Prop_9 + Green_Prop_9 + Blue_Prop_9 + Dispersion_9 + Hue_Shifted_9

#formula <- BT ~ Red + Green + Blue + Red_Prop + Green_Prop + Blue_Prop + Dispersion + Red_9 + Green_9 + Blue_9 + Red_Prop_9 + Green_Prop_9 + Blue_Prop_9 + Dispersion_9

#formula <- BT ~ Red + Green + Blue + Red_9 + Green_9 + Blue_9

rec <- recipe(formula, data = train_data) %>%
  step_normalize(all_numeric_predictors())

set.seed(1) # for reproducibility
resamples <- vfold_cv(train_data, v=10, strata=BT)
cv_control <- control_resamples(save_pred=TRUE)

tune_logreg_spec <- logistic_reg(engine="glmnet", mode="classification",
                                 penalty=tune(), mixture=tune())

tune_logreg_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(tune_logreg_spec)
logreg_params <- extract_parameter_set_dials(tune_logreg_wf) %>%
  update(
    penalty=penalty(c(-2, -0.5)),
    mixture=mixture(c(0, 1)))

tune_results_logreg <- tune_grid(tune_logreg_wf,
                                 resamples=resamples,
                                 control=cv_control,
                                 metrics = metric_set(roc_auc, accuracy),
                                 grid=grid_random(logreg_params, size=50))

autoplot(tune_results_logreg)
```

```{r select best}

# Extract best parameters from the tuning results object
best_logreg <- select_best(tune_results_logreg, metric = "roc_auc")

# Finalize the workflow with the best parameters
final_logreg_fit <- finalize_workflow(tune_logreg_wf, best_logreg) %>% 
  fit(data = train_data)

# Extract the model coefficients
tidy(final_logreg_fit)
```

```{r cross validation approach}
# Define cross-validation approach
set.seed(1)

resamples <- vfold_cv(train_data, v = 10, strata = BT)
custom_metrics <- metric_set(roc_auc, accuracy, precision, f_meas)
cv_control <- control_resamples(save_pred = TRUE)
```

```{r knn with pca}
library(tidymodels)
library(tune)
library(ggplot2)

# Define the model formula (using all predictors including PCA-tuned ones)
knn_formula <- BT ~ Red + Green + Blue + Luminance + a + b + Hue + Saturation + Value +
  Red_Prop + Green_Prop + Blue_Prop + Dispersion + Hue_Shifted +
  Red_9 + Green_9 + Blue_9 + Luminance_9 + a_9 + b_9 + Hue_9 + Saturation_9 + Value_9 +
  Red_Prop_9 + Green_Prop_9 + Blue_Prop_9 + Dispersion_9 + Hue_Shifted_9

# Build a recipe: normalize all numeric predictors then apply PCA with tunable components
rec <- recipe(knn_formula, data = train_data) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_pca(all_predictors(), num_comp = tune())

# Define 10-fold cross-validation using stratification on BT
set.seed(1)
resamples <- vfold_cv(train_data, v = 10, strata = BT)
cv_control <- control_resamples(save_pred = TRUE)

# Specify the k-NN model with tunable neighbors
tune_knn_spec <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

# Create a workflow with the recipe and model
tune_knn_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(tune_knn_spec)

# Extract tunable parameters; here we tune both 'neighbors' and the number of PCA components
knn_params <- extract_parameter_set_dials(tune_knn_wf) %>%
  update(
    neighbors = neighbors(range = c(1, 100)),
    num_comp = num_comp(range = c(1, 28))
  )

# First pass: Grid tuning over a regular grid (e.g., 5 levels for each parameter)
set.seed(1)
grid_results_knn <- tune_grid(
  tune_knn_wf,
  resamples = resamples,
  grid = grid_regular(knn_params, levels = 5),
  metrics = metric_set(roc_auc, accuracy),
  control = cv_control
)
```

```{r knn autoplot 1}
autoplot(grid_results_knn)
```

```{r knn second pass bayes}
# Second pass: Bayesian optimization using the grid tuning results as initial design;
# here we run 20 iterations of Bayesian tuning.
set.seed(1)
bayes_results_knn <- tune_bayes(
  tune_knn_wf,
  resamples = resamples,
  initial = grid_results_knn,
  param_info = knn_params,
  iter = 5,
  metrics = metric_set(roc_auc, accuracy),
  control = control_bayes(verbose = TRUE, save_pred = TRUE)
)

# Select the best parameters (for example, using roc_auc as the primary metric)
best_knn <- select_best(bayes_results_knn, metric = "roc_auc")

# Finalize the workflow with the best parameters, and fit the model to the full training data
final_knn_wf <- finalize_workflow(tune_knn_wf, best_knn)
final_knn_fit <- final_knn_wf %>% fit(data = train_data)

# View the final model (the underlying fitted model can be extracted with extract_fit_parsnip())
final_knn_fit
```

```{r autoplot bayes knn}
autoplot(bayes_results_knn)
```

```{r set formulas}
# RGB Model Formula and Recipe
rgb_formula <- BT ~ Red + Green + Blue
rgb_recipe <- recipe(rgb_formula, data = train_data)

# CIELab Model Formula and Recipe
lab_formula <- BT ~ Luminance + a + b
lab_recipe <- recipe(lab_formula, data = train_data)

# HSV Model Formula and Recipe
hsv_formula <- BT ~ Hue + Saturation + Value
hsv_recipe <- recipe(hsv_formula, data = train_data)
```

```{r specify models}
# Specify models
logreg_spec <- logistic_reg(mode="classification", engine="glm")
```

```{r define workflows}
# RGB Models
logreg_rgb_wf <- workflow() %>% add_recipe(rgb_recipe) %>% add_model(logreg_spec)

# CIELab Models
logreg_lab_wf <- workflow() %>% add_recipe(lab_recipe) %>% add_model(logreg_spec)

# HSV Models
logreg_hsv_wf <- workflow() %>% add_recipe(hsv_recipe) %>% add_model(logreg_spec)
```

```{r cross validate tuned}
# Finalize the workflow using the best parameters
final_logreg_wf <- finalize_workflow(tune_logreg_wf, best_logreg)

# Generate a cross-validation object from the finalized workflow
final_logreg_cv <- fit_resamples(
  final_logreg_wf,
  resamples = resamples,
  metrics = custom_metrics,
  control = cv_control
)
```

```{r cross validate logreg}
# Cross-validation for RGB models
logreg_rgb_cv <- fit_resamples(logreg_rgb_wf, resamples, metrics = custom_metrics, control = cv_control)

# Cross-validation for CIELab models
logreg_lab_cv <- fit_resamples(logreg_lab_wf, resamples, metrics = custom_metrics, control = cv_control)

# Cross-validation for HSV models
logreg_hsv_cv <- fit_resamples(logreg_hsv_wf, resamples, metrics = custom_metrics, control = cv_control)
```

```{r fit final models on train}
# Fit final models on train_data for RGB
final_logreg_rgb_fit <- logreg_rgb_wf %>% fit(data = train_data)

# Fit final models on train_data for CIELab
final_logreg_lab_fit <- logreg_lab_wf %>% fit(data = train_data)

# Fit final models on train_data for HSV
final_logreg_hsv_fit <- logreg_hsv_wf %>% fit(data = train_data)
```

```{r autplot roc}
# Obtain predictions for each model on training data
pred_logreg     <- augment(final_logreg_fit, new_data = train_data)
pred_logreg_rgb <- augment(final_logreg_rgb_fit, new_data = train_data)
pred_logreg_lab <- augment(final_logreg_lab_fit, new_data = train_data)
pred_logreg_hsv <- augment(final_logreg_hsv_fit, new_data = train_data)

# Compute ROC curves for each model (using .pred_TRUE as the estimated probability)
roc_logreg     <- roc_curve(pred_logreg, truth = BT, .pred_TRUE, event_level = "first") %>% mutate(model = "Logistic Regression")
roc_logreg_rgb <- roc_curve(pred_logreg_rgb, truth = BT, .pred_TRUE, event_level = "first") %>% mutate(model = "Logistic Regression (RGB)")
roc_logreg_lab <- roc_curve(pred_logreg_lab, truth = BT, .pred_TRUE, event_level = "first") %>% mutate(model = "Logistic Regression (CIELab)")
roc_logreg_hsv <- roc_curve(pred_logreg_hsv, truth = BT, .pred_TRUE, event_level = "first") %>% mutate(model = "Logistic Regression (HSV)")

# Combine all ROC curves
roc_all <- bind_rows(roc_logreg, roc_logreg_rgb, roc_logreg_lab, roc_logreg_hsv)

# Option 2: Using yardstick's autoplot (if it supports the grouping variable)
autoplot(roc_all) +
  labs(title = "Overlay of ROC Curves for Logistic Regression Models",
       x = "1 - Specificity",
       y = "Sensitivity",
       color = "Model")
```

```{r threshold graphs 1}
threshold_graph <- function(model_cv, model_name) {
    performance <- probably::threshold_perf(collect_predictions(model_cv), BT, .pred_TRUE,
        thresholds=seq(0.01, 0.99, 0.01), event_level="first",
        metrics=metric_set(f_meas, accuracy, sens))
    max_metrics <- performance %>%
        drop_na() %>%
        group_by(.metric) %>%
        filter(.estimate == max(.estimate))
    g <- ggplot(performance, aes(x=.threshold, y=.estimate, color=.metric)) +
        geom_line() +
        geom_point(data=max_metrics, color="black") +
        labs(title=model_name, x="Threshold", y="Metric value") +
        coord_cartesian(ylim=c(0, 1))
    thresholds <- max_metrics %>%
        select(.metric, .threshold) %>%
        deframe()
    return(list(graph=g, thresholds=thresholds))
}

visualize_conf_mat <- function(model_cv, thresholds, metric) {
    threshold <- thresholds[metric]
    cm <- collect_predictions(model_cv) %>%
        mutate(
            .pred_class = make_two_class_pred(.pred_TRUE, c("TRUE", "FALSE"), threshold=threshold),
        ) %>%
        conf_mat(truth=BT, estimate=.pred_class)
    autoplot(cm, type="heatmap") +
        labs(title=sprintf("Threshold %.2f (%s)", threshold, metric))
}

overview_model <- function(model_cv, model_name) {
    tg <- threshold_graph(model_cv, model_name)
    g1 <- visualize_conf_mat(model_cv, tg$thresholds, "accuracy")
    g2 <- visualize_conf_mat(model_cv, tg$thresholds, "f_meas")
    g3 <- visualize_conf_mat(model_cv, tg$thresholds, "sens")
    tg$graph + (g1 / g2 / g3)
}
```

```{r threshold measure apply function}
# RGB Models
rgb_g1 <- overview_model(logreg_rgb_cv, "Logistic Regression (RGB)")

# CIELab Models
lab_g1 <- overview_model(logreg_lab_cv, "Logistic Regression (CIELab)")

# HSV Models
hsv_g1 <- overview_model(logreg_hsv_cv, "Logistic Regression (HSV)")

#Tuned Model
tuned_g1 <- overview_model(final_logreg_cv, "Tuned Logistic Regression (Elastic Net)")
```

```{r print first threshold imgs}
#| fig.width: 10
#| fig.height: 15
#| out.width: 100%
#| fig.cap: Metrics as a function of threshold optimization across all color spaces.
#| warning: FALSE
# Arrange in 3 rows (RGB, CIELab, HSV)
combined_threshold_plots <- (rgb_g1) /
                            (lab_g1) /
                            (hsv_g1) /
                            (tuned_g1)

# Print the combined plot
combined_threshold_plots

ggsave("combined_threshold_plots_elastic.png", plot = combined_threshold_plots, width = 10, height = 15, dpi = 600)
```

```{r compare roc-auc cv and full train}
compute_roc_diff <- function(cv_object, workflow, train_data) {
  # Cross-validation ROC-AUC (mean value)
  cv_roc <- collect_metrics(cv_object) %>%
    filter(.metric == "roc_auc") %>%
    pull(mean)
  
  # Fit the model
  final_fit <- workflow %>% fit(train_data)
  
  # Evaluate ROC-AUC
  full_preds <- augment(final_fit, new_data = train_data)
  
  full_roc <- roc_auc(full_preds, truth = BT, .pred_TRUE, event_level = "first") %>%
    pull(.estimate)
  
  # Return a tibble with ROC values and their difference
  tibble(
    cv_roc = cv_roc,
    full_roc = full_roc,
    diff = full_roc - cv_roc
  )
}

roc_diff_results <- bind_rows(
  compute_roc_diff(logreg_rgb_cv, logreg_rgb_wf, train_data) %>% 
    mutate(model = "Logistic Regression", color_space = "RGB"),
  
  compute_roc_diff(logreg_lab_cv, logreg_lab_wf, train_data) %>% 
    mutate(model = "Logistic Regression", color_space = "CIELab"),
  
  compute_roc_diff(logreg_hsv_cv, logreg_hsv_wf, train_data) %>% 
    mutate(model = "Logistic Regression", color_space = "HSV"),
  
   compute_roc_diff(final_logreg_cv, final_logreg_wf, train_data) %>% 
    mutate(model = "Tuned Logistic Regression (Elastic Net)", color_space = "All Predictor Variables")
)

roc_diff_results <- roc_diff_results %>%
  mutate(
    color_space = factor(color_space, levels = c("RGB", "CIELab", "HSV", "All Predictor Variables")),
    model = factor(model, levels = c("Logistic Regression", "Tuned Logistic Regression (Elastic Net)"))
  ) %>%
  arrange(color_space, model) %>%
  select(color_space, model, cv_roc, full_roc, diff) %>%
  kable(
    caption = "Comparison of ROC-AUC between cross-validation and full-training fits",
    digits = 6,
    col.names = c("Color Space", "Model", "ROC-AUC of CV Folds", "ROC-AUC of Fitted Model", "Difference")
  ) %>%
  kable_styling(full_width = FALSE) %>%
  collapse_rows(columns = 1, valign = "top")

roc_diff_results
```

```{r save roc cv kable}
save_kable(roc_diff_results, file = "roc_diff_results_elastic.png", zoom = 2)
```

```{r optimal thresholds}
# Find optimal thresholds
threshold_scan <- function(model, data, model_name) {
  threshold_data <- model %>%
    augment(data) %>%
    probably::threshold_perf(
      truth = BT,
      estimate = .pred_TRUE,
      thresholds = seq(0.01, 0.99, 0.01),
      event_level = "first",
      metrics = metric_set(f_meas)
    )
  opt_threshold <- threshold_data %>%
    drop_na() %>%
    arrange(desc(.estimate)) %>%
    slice(1)
  list(
    threshold = opt_threshold$.threshold,
    threshold_data = threshold_data,
    opt_threshold = opt_threshold,
    model_name = model_name
  )
}
```

```{r scan threshold holdout}
# For RGB model:
if (file.exists("logreg_rgb_result.rds")) {
  logreg_rgb_result <- readRDS("logreg_rgb_result.rds")
} else {
  logreg_rgb_result <- threshold_scan(final_logreg_rgb_fit, holdout_data, "Logistic Regression (RGB)")
  saveRDS(logreg_rgb_result, "logreg_rgb_result.rds")
}

# For CIELab model:
if (file.exists("logreg_lab_result.rds")) {
  logreg_lab_result <- readRDS("logreg_lab_result.rds")
} else {
  logreg_lab_result <- threshold_scan(final_logreg_lab_fit, holdout_data, "Logistic Regression (CIELab)")
  saveRDS(logreg_lab_result, "logreg_lab_result.rds")
}

# For HSV model:
if (file.exists("logreg_hsv_result.rds")) {
  logreg_hsv_result <- readRDS("logreg_hsv_result.rds")
} else {
  logreg_hsv_result <- threshold_scan(final_logreg_hsv_fit, holdout_data, "Logistic Regression (HSV)")
  saveRDS(logreg_hsv_result, "logreg_hsv_result.rds")
}

# For Tuned Logistic Regression (Elastic Net):
if (file.exists("logreg_tune_result.rds")) {
  logreg_tune_result <- readRDS("logreg_tune_result.rds")
} else {
  logreg_tune_result <- threshold_scan(final_logreg_fit, holdout_data, "Tuned Logistic Regression (Elastic Net)")
  saveRDS(logreg_tune_result, "logreg_tune_result.rds")
}
```

```{r optimal thresholds set}
# Optimal thresholds
logreg_rgb_holdout_threshold <- logreg_rgb_result$threshold

logreg_lab_holdout_threshold <- logreg_lab_result$threshold

logreg_hsv_holdout_threshold <- logreg_hsv_result$threshold

logreg_tune_holdout_threshold <- logreg_tune_result$threshold
```

```{r combine threshold graphs}
#| fig.width: 6
#| fig.height: 9
#| fig.cap: F-Measure by threshold for each model and color space
#| warning: FALSE

# Function to plot threshold performance
plot_threshold <- function(result) {
  ggplot(result$threshold_data, aes(x = .threshold, y = .estimate)) +
    geom_line() +
    geom_point(data = result$opt_threshold, color = "red", size = 2) +
    labs(title = result$model_name, x = "Threshold", y = "F-Measure") +
    coord_cartesian(ylim = c(0, 1))
}

## RGB
g_logreg_rgb <- plot_threshold(logreg_rgb_result)

## CIELab
g_logreg_lab <- plot_threshold(logreg_lab_result)

## HSV
g_logreg_hsv <- plot_threshold(logreg_hsv_result)

## Tuned
g_logreg_tune <- plot_threshold(logreg_tune_result)

# Combine plots
combined_thresholds <- (g_logreg_rgb) /
                       (g_logreg_lab) /
                       (g_logreg_hsv) /
                        (g_logreg_tune) +
                       plot_annotation(title = "Threshold Performance (F-Measure) Across Color Spaces")

combined_thresholds

ggsave("combined_thresholds_tune.png", plot = combined_thresholds, width = 6, height = 9, dpi = 600)
```

```{r scan threshold cv}
threshold_scan_cv <- function(cv_obj, model_name) {
  threshold_data <- cv_obj %>%
    collect_predictions() %>%
    probably::threshold_perf(
      truth = BT,
      estimate = .pred_TRUE,
      thresholds = seq(0.05, 0.95, 0.01),
      event_level = "first",
      metrics = metric_set(f_meas)
    )
  opt_threshold <- threshold_data %>%
    drop_na() %>%
    arrange(desc(.estimate)) %>%
    slice(1)
  list(
    threshold = opt_threshold$.threshold
  )
}

# Compute thresholds

## RGB Models
logreg_rgb_train_result <- threshold_scan_cv(logreg_rgb_cv, "Logistic Regression (RGB)")

## CIELab Models
logreg_lab_train_result <- threshold_scan_cv(logreg_lab_cv, "Logistic Regression (CIELab)")

## HSV Models
logreg_hsv_train_result <- threshold_scan_cv(logreg_hsv_cv, "Logistic Regression (HSV)")

## Tuned Models
logreg_tune_train_result <- threshold_scan_cv(final_logreg_cv, "Tuned Logistic Regression (Elastic Net)")

# Extract optimal thresholds

## RGB
logreg_rgb_train_threshold <- logreg_rgb_train_result$threshold

## CIELab
logreg_lab_train_threshold <- logreg_lab_train_result$threshold

## HSV
logreg_hsv_train_threshold <- logreg_hsv_train_result$threshold

## Tune
logreg_tune_train_threshold <- logreg_tune_train_result$threshold
```

```{r model evaluation functions}
predict_at_threshold <- function(model, data, threshold) {
  model %>%
    augment(data) %>%
    mutate(
      .pred_class = make_two_class_pred(
        .pred_TRUE,
        c("TRUE", "FALSE"),
        threshold = threshold
      )
    )
}

calculate_metrics_at_threshold <- function(
  model,
  train,
  holdout,
  model_name,
  color_space,
  train_threshold,
  holdout_threshold
) {
  bind_rows(
    # Metrics for the training set
    bind_cols(
      color_space = color_space,
      model = model_name,
      dataset = "train",
      threshold = train_threshold,
      metrics(predict_at_threshold(model, train, train_threshold), truth = BT, estimate = .pred_class)
    ),
    bind_cols(
      color_space = color_space,
      model = model_name,
      dataset = "train",
      threshold = train_threshold,
      roc_auc(model %>% augment(train), truth = BT, .pred_TRUE, event_level = "first")
    ),
    bind_cols(
      color_space = color_space,
      model = model_name,
      dataset = "train",
      threshold = train_threshold,
      f_meas(predict_at_threshold(model, train, train_threshold), truth = BT, estimate = .pred_class)
    ),
    bind_cols(
      color_space = color_space,
      model = model_name,
      dataset = "train",
      threshold = train_threshold,
      sens(predict_at_threshold(model, train, train_threshold), truth = BT, estimate = .pred_class)
    ),
    # Metrics for the holdout set
    bind_cols(
      color_space = color_space,
      model = model_name,
      dataset = "holdout",
      threshold = holdout_threshold,
      metrics(predict_at_threshold(model, holdout, holdout_threshold), truth = BT, estimate = .pred_class)
    ),
    bind_cols(
      color_space = color_space,
      model = model_name,
      dataset = "holdout",
      threshold = holdout_threshold,
      roc_auc(model %>% augment(holdout), BT, .pred_TRUE, event_level = "first")
    ),
    bind_cols(
      color_space = color_space,
      model = model_name,
      dataset = "holdout",
      threshold = holdout_threshold,
      f_meas(predict_at_threshold(model, holdout, holdout_threshold), truth = BT, estimate = .pred_class)
    ),
    bind_cols(
      color_space = color_space,
      model = model_name,
      dataset = "holdout",
      threshold = holdout_threshold,
      sens(predict_at_threshold(model, holdout, holdout_threshold), truth = BT, estimate = .pred_class)
    )
  )
}
```

```{r metrics get}
metrics_at_threshold <- bind_rows(
  # RGB Models
  calculate_metrics_at_threshold(
    final_logreg_rgb_fit, train_data, holdout_data,
    "Logistic Regression", "RGB",
    logreg_rgb_train_threshold, logreg_rgb_holdout_threshold
  ),
  
  # CIELab Models
  calculate_metrics_at_threshold(
    final_logreg_lab_fit, train_data, holdout_data,
    "Logistic Regression", "CIELab",
    logreg_lab_train_threshold, logreg_lab_holdout_threshold
  ),
  
  # HSV Models
  calculate_metrics_at_threshold(
    final_logreg_hsv_fit, train_data, holdout_data,
    "Logistic Regression", "HSV",
    logreg_hsv_train_threshold, logreg_hsv_holdout_threshold
  ),
  
  # Tuned Models
  calculate_metrics_at_threshold(
    final_logreg_fit, train_data, holdout_data,
    "Tuned Logistic Regression (Elastic Net)", "All Predictors",
    logreg_tune_train_threshold, logreg_tune_holdout_threshold
  )
  
) %>%
  arrange(dataset, color_space, model)
```

```{r model eval table}
metrics_at_threshold_tune <- metrics_at_threshold %>%
  mutate(
    dataset = factor(dataset, levels = c("train", "holdout")),
    color_space = factor(color_space, levels = c("RGB", "CIELab", "HSV", "All Predictors")),
    model = factor(model, levels = c("Logistic Regression", "LDA", "QDA", "Tuned Logistic Regression (Elastic Net)"))
  ) %>%
  tidyr::pivot_wider(names_from = .metric, values_from = .estimate) %>%
  select(
    dataset, color_space, model, threshold, 
    accuracy, roc_auc, sens, f_meas
  ) %>%
  arrange(dataset, color_space, model, threshold) %>%
  knitr::kable(
    caption = "Final metrics for models at chosen thresholds.",
    digits = 4
  ) %>%
  kableExtra::kable_styling(full_width = FALSE) %>%
  kableExtra::collapse_rows(columns = 1:2, valign = "top")

metrics_at_threshold_tune

save_kable(metrics_at_threshold_tune, file = "metrics_at_threshold_tune.png", zoom = 2)
```