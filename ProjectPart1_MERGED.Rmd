---
title: "Project Part 1"
author: "Virginia Brame, Clay Harris, Hai Liu"
date: "`r Sys.Date()`"
output:
  pdf_document: default
header-includes:
  - \usepackage{float}
---
```{r setup}
knitr::opts_chunk$set(
  echo = TRUE,
  cache = TRUE,
  autodep = TRUE,
  fig.align = "center",
  fig.pos = "H",
  out.width = "100%"
)
```

## Data (loading, wrangling, EDA)

```{r parallel}
#| cache: FALSE
#| message: FALSE
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
```

```{r libraries}
#| cache: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(tidymodels)
library(discrim)
library(leaflet)
library(terra)
library(htmlwidgets)
library(leafem)
library(colordistance)
library(jpeg)
library(patchwork)
library(probably)
library(gridExtra)
library(plotly)
library(mapview)
library(farver)
```

### Data loading and wrangling

Since we are only interested in the level of "Blue Tarp", I create a new variable `BT` with only two classes, i.e., "TRUE" for "Blue Tarp" and "FALSE" for everything else.

```{r holdout data processing}
#| message: FALSE
#| warning: FALSE

col_names <- c('ID','X','Y','Map X','Map Y','Lat','Lon','Red','Green','Blue')

blue_files <- c(
  "orthovnir069_ROI_Blue_Tarps.txt",
  "orthovnir067_ROI_Blue_Tarps.txt",
  "orthovnir078_ROI_Blue_Tarps.txt"
)

non_blue_files <- c(
  "orthovnir057_ROI_NON_Blue_Tarps.txt",
  "orthovnir078_ROI_NON_Blue_Tarps.txt",
  "orthovnir067_ROI_NOT_Blue_Tarps.txt",
  "orthovnir069_ROI_NOT_Blue_Tarps.txt"
)

blue_data <- map_dfr(blue_files, ~ 
  read_table(.x, comment = ";", col_names = col_names) %>% 
    select(Lat, Lon, Red, Green, Blue) %>% 
    mutate(BT = "TRUE")
)

non_blue_data <- map_dfr(non_blue_files, ~ 
  read_table(.x, comment = ";", col_names = col_names) %>% 
    select(Lat, Lon, Red, Green, Blue) %>% 
    mutate(BT = "FALSE")
)

holdout_data <- bind_rows(blue_data, non_blue_data) %>% 
  mutate(BT = factor(BT, levels = c("TRUE", "FALSE")))
```

```{r training data processing}
#| message: FALSE

train_data <- read_csv("HaitiPixels.csv") %>%
  mutate(BT = factor(if_else(Class == "Blue Tarp", "TRUE", "FALSE"), levels = c("TRUE", "FALSE"))) %>%
  select(Red, Green, Blue, BT)
```

```{r add CIELab and HSV}
convert_color_spaces <- function(data) {
  # Convert RGB to CIELab
  lab_values <- farver::convert_colour(
    as.matrix(data[, c("Red", "Green", "Blue")]), 
    from = "rgb", 
    to = "lab"
  )
  
  # Convert RGB to HSV
  hsv_values <- farver::convert_colour(
    as.matrix(data[, c("Red", "Green", "Blue")]), 
    from = "rgb", 
    to = "hsv"
  )
  
  # Convert
  lab_df <- as.data.frame(lab_values)
  colnames(lab_df) <- c("Luminance", "a", "b") 
  hsv_df <- as.data.frame(hsv_values)
  colnames(hsv_df) <- c("Hue", "Saturation", "Value")
  
  # Bind new columns
  data <- cbind(data, lab_df, hsv_df)
  
  return(data)
}

# Apply function
train_data <- convert_color_spaces(train_data)
holdout_data <- convert_color_spaces(holdout_data)
```

### EDA

```{r prepare spatial data}
# Convert
holdout_data_sp <- holdout_data %>% 
  rename(x = Lon, y = Lat)
v <- terra::vect(holdout_data_sp, geom = c("x", "y"), crs = "EPSG:4326")

# Reproject to UTM (meters)
v_utm <- terra::project(v, "EPSG:32618")

# Create an empty raster
r_empty <- terra::rast(terra::ext(v_utm), resolution = 0.5, crs = "EPSG:32618")

# Rasterize
r_b1 <- terra::rasterize(v_utm, r_empty, field = "Red", overwrite = TRUE)
r_b2 <- terra::rasterize(v_utm, r_empty, field = "Green", overwrite = TRUE)
r_b3 <- terra::rasterize(v_utm, r_empty, field = "Blue", overwrite = TRUE)

# Combine
rgb_raster <- c(r_b1, r_b2, r_b3)

# Reproject
rgb_raster_wgs <- terra::project(rgb_raster, "EPSG:4326", overwrite = TRUE)

# Convert to brick for leaflet
rgb_brick <- raster::brick(rgb_raster_wgs)
```

```{r create map}
# Create map
m <- leaflet(options = leafletOptions(maxZoom = 25)) %>%
  addTiles(options = tileOptions(maxZoom = 25)) %>%
  leafem::addRasterRGB(rgb_brick, r = 1, g = 2, b = 3)

if (knitr::is_html_output()) {
  htmlwidgets::saveWidget(m, "interactive_map.html")
  htmltools::includeHTML("interactive_map.html")
} else {
  # Save a static image for PDF output
  mapshot(m, file = "map_static.png")
  knitr::include_graphics("map_static.png")
}
```

```{r calculate area}
# Identify non-null pixels
non_na_mask <- !is.na(r_b1) | !is.na(r_b2) | !is.na(r_b3)

# Count cells
non_na_cells <- sum(terra::values(non_na_mask), na.rm = TRUE)

# Area of one pixel
cell_area_km2 <- (terra::res(r_empty)[1] * terra::res(r_empty)[2]) / 1e6

# Total area
total_area_km2 <- non_na_cells * cell_area_km2

# Print the result
total_area_km2
```


```{r density plots}
rgb1 <- train_data %>% 
  ggplot(aes(x=Red,fill=BT))+ 
  geom_density(alpha=0.4)+ 
  scale_fill_manual(values=c("cyan", "tomato"))

rgb2 <- train_data %>% 
  ggplot(aes(x=Green,fill=BT))+ 
  geom_density(alpha=0.4)+ 
  scale_fill_manual(values=c("cyan", "tomato"))

rgb3 <- train_data %>% 
  ggplot(aes(x=Blue,fill=BT))+ 
  geom_density(alpha=0.4)+ 
  scale_fill_manual(values=c("cyan", "tomato"))

grid.arrange(rgb1, rgb2, rgb3, top = "BT assignment by color layer | Training")
```

```{r}
rgb4 <- holdout_data %>% 
  ggplot(aes(x=Red,fill=BT))+ 
  geom_density(alpha=0.4)+ 
  scale_fill_manual(values=c("cyan", "tomato"))

rgb5 <- holdout_data %>% 
  ggplot(aes(x=Green,fill=BT))+ 
  geom_density(alpha=0.4)+ 
  scale_fill_manual(values=c("cyan", "tomato"))

rgb6 <- holdout_data %>% 
  ggplot(aes(x=Blue,fill=BT))+ 
  geom_density(alpha=0.4)+ 
  scale_fill_manual(values=c("cyan", "tomato"))

grid.arrange(rgb4, rgb5, rgb6, top = "BT assignment by color layer | Holdout")
```

Understanding that:
-   Blue is 0,0,255
-   Green is 0,255,0
-   Red is 255,0,0

```{r plotly plot}
plot_ly(train_data, x = ~Red, y = ~Green, z = ~Blue, color = ~BT, 
        colors = c("orange", "lightblue")) %>%
  add_markers() %>%
  layout(title = "Test Data | 3D RGB Plot by BlueTarp",
         scene = list(xaxis = list(title = 'Red'),
                      yaxis = list(title = 'Green'),
                      zaxis = list(title = 'Blue')))
```
Here we can clearly see the separation of the two levels of BlueTarp in the training data.  

```{r makeshift village}
#| message: FALSE
image_path <- "orthovnir078_makeshift_villiage1.jpg"
colordistance::plotPixels(image_path)

H8hist <- colordistance::getImageHist(image_path, bins=c(1, 1, 2))
```

```{r create jpg holdout}
# Number of pixels
n <- nrow(holdout_data)
maxHeight <- 65500
height <- min(n, maxHeight)
width <- ceiling(n / height)
total_pixels <- height * width

# Normalize RGB
r <- holdout_data$Red / 255
g <- holdout_data$Green / 255
b <- holdout_data$Blue / 255

# Calculate padding
pad <- total_pixels - n

# Pad
if(pad > 0){
  r <- c(r, rep(0, pad))
  g <- c(g, rep(1, pad))
  b <- c(b, rep(0, pad))
}

# Make array
img_array <- array(c(matrix(r, nrow = height, ncol = width),
                     matrix(g, nrow = height, ncol = width),
                     matrix(b, nrow = height, ncol = width)),
                   dim = c(height, width, 3))

# Write to jpg
writeJPEG(img_array, target = "holdout_colors.jpg")
```

```{r holdout color plot}
image_path <- "holdout_colors.jpg"
colordistance::plotPixels(image_path)
```

```{r}
# Number of pixels
n <- nrow(train_data)
maxHeight <- 65500
height <- min(n, maxHeight)
width <- ceiling(n / height)
total_pixels <- height * width

# Normalize RGB
r <- train_data$Red / 255
g <- train_data$Green / 255
b <- train_data$Blue / 255

# Calculate padding
pad <- total_pixels - n

# Pad
if(pad > 0){
  r <- c(r, rep(0, pad))
  g <- c(g, rep(1, pad))
  b <- c(b, rep(0, pad))
}

# Make array
img_array <- array(c(matrix(r, nrow = height, ncol = width),
                     matrix(g, nrow = height, ncol = width),
                     matrix(b, nrow = height, ncol = width)),
                   dim = c(height, width, 3))

# Write to jpg
writeJPEG(img_array, target = "train_colors.jpg")
```

```{r}
image_path <- "train_colors.jpg"
colordistance::plotPixels(image_path)
```

```{r}
bt_data <- holdout_data %>% 
  filter(BT == "TRUE")

n <- nrow(bt_data)
maxHeight <- 65500
height <- min(n, maxHeight)
width <- ceiling(n / height)
total_pixels <- height * width

r <- bt_data$Red / 255
g <- bt_data$Green / 255
b <- bt_data$Blue / 255

pad <- total_pixels - n

if(pad > 0){
  r <- c(r, rep(0, pad))
  g <- c(g, rep(1, pad))
  b <- c(b, rep(0, pad))
}

img_array <- array(
  c(matrix(r, nrow = height, ncol = width),
    matrix(g, nrow = height, ncol = width),
    matrix(b, nrow = height, ncol = width)),
  dim = c(height, width, 3)
)

writeJPEG(img_array, target = "holdout_BT.jpg")
```

```{r}
image_path <- "holdout_BT.jpg"
H8hist <- colordistance::getImageHist(image_path, bins=c(1, 1, 2))
```

```{r}
bt_data <- train_data %>% 
  filter(BT == "TRUE")

n <- nrow(bt_data)
maxHeight <- 65500
height <- min(n, maxHeight)
width <- ceiling(n / height)
total_pixels <- height * width

r <- bt_data$Red / 255
g <- bt_data$Green / 255
b <- bt_data$Blue / 255

pad <- total_pixels - n

if(pad > 0){
  r <- c(r, rep(0, pad))
  g <- c(g, rep(1, pad))
  b <- c(b, rep(0, pad))
}

img_array <- array(
  c(matrix(r, nrow = height, ncol = width),
    matrix(g, nrow = height, ncol = width),
    matrix(b, nrow = height, ncol = width)),
  dim = c(height, width, 3)
)

writeJPEG(img_array, target = "train_BT.jpg")
```

```{r}
image_path <- "train_BT.jpg"
H8hist <- colordistance::getImageHist(image_path, bins=c(1, 1, 2))
```

Have a look at the distributioin of the two classes for the outcome named "BT" (for BlueTarp).
```{r}
#| fig.cap: Distribution of Blue Tarp among all the observations.
#| fig.width: 5
#| fig.height: 4
#| fig.align: center
#| out.width: 60%
train_data |> 
    ggplot(aes(x=BT, fill=BT)) +
    geom_bar(position="dodge")
```

I can see that the two outcome classes are extremely unbalanced. I will keep this in mind and deal with it later.

## Methods

### Build three classification models, *i.e.*, LDA, QDA, and logistic regression, with cross-validation.

#### Prepare model workflows

Define the preprocessing steps. In this case, we normalize all numeric predictors.
```{r set formulas}
# RGB Model Formula and Recipe
rgb_formula <- BT ~ Red + Green + Blue
rgb_recipe <- recipe(rgb_formula, data = train_data)

# CIELab Model Formula and Recipe
lab_formula <- BT ~ Luminance + a + b
lab_recipe <- recipe(lab_formula, data = train_data)

# HSV Model Formula and Recipe
hsv_formula <- BT ~ Hue + Saturation + Value
hsv_recipe <- recipe(hsv_formula, data = train_data)
```

Specify the three models.
```{r specify models}
# Specify models
logreg_spec <- logistic_reg(mode="classification", engine="glm")
lda_spec <- discrim_linear(mode="classification", engine="MASS")
qda_spec <- discrim_quad(mode="classification", engine="MASS")
```

Combine preprocessing steps and model specification in workflow.
```{r define workflows}
# RGB Models
logreg_rgb_wf <- workflow() %>% add_recipe(rgb_recipe) %>% add_model(logreg_spec)
lda_rgb_wf <- workflow() %>% add_recipe(rgb_recipe) %>% add_model(lda_spec)
qda_rgb_wf <- workflow() %>% add_recipe(rgb_recipe) %>% add_model(qda_spec)

# CIELab Models
logreg_lab_wf <- workflow() %>% add_recipe(lab_recipe) %>% add_model(logreg_spec)
lda_lab_wf <- workflow() %>% add_recipe(lab_recipe) %>% add_model(lda_spec)
qda_lab_wf <- workflow() %>% add_recipe(lab_recipe) %>% add_model(qda_spec)

# HSV Models
logreg_hsv_wf <- workflow() %>% add_recipe(hsv_recipe) %>% add_model(logreg_spec)
lda_hsv_wf <- workflow() %>% add_recipe(hsv_recipe) %>% add_model(lda_spec)
qda_hsv_wf <- workflow() %>% add_recipe(hsv_recipe) %>% add_model(qda_spec)
```


#### Cross-validation

Define cross-validation approach 
- 10-fold cross-validation using stratified sampling
- Measure performance using ROC-AUC (we also collect accuracy)
- Save resample predictions, so that we can build ROC curves using cross-validation results
```{r cross validation approach}
# Define cross-validation approach
set.seed(6030)

resamples <- vfold_cv(train_data, v = 10, strata = BT)
custom_metrics <- metric_set(roc_auc, accuracy, precision, f_meas)
cv_control <- control_resamples(save_pred = TRUE)
```

Cross-validation
```{r cross validate}
#| message: FALSE
#| warning: FALSE

# Cross-validation for RGB models
logreg_rgb_cv <- fit_resamples(logreg_rgb_wf, resamples, metrics = custom_metrics, control = cv_control)
lda_rgb_cv <- fit_resamples(lda_rgb_wf, resamples, metrics = custom_metrics, control = cv_control)
qda_rgb_cv <- fit_resamples(qda_rgb_wf, resamples, metrics = custom_metrics, control = cv_control)

# Cross-validation for CIELab models
logreg_lab_cv <- fit_resamples(logreg_lab_wf, resamples, metrics = custom_metrics, control = cv_control)
lda_lab_cv <- fit_resamples(lda_lab_wf, resamples, metrics = custom_metrics, control = cv_control)
qda_lab_cv <- fit_resamples(qda_lab_wf, resamples, metrics = custom_metrics, control = cv_control)

# Cross-validation for HSV models
logreg_hsv_cv <- fit_resamples(logreg_hsv_wf, resamples, metrics = custom_metrics, control = cv_control)
lda_hsv_cv <- fit_resamples(lda_hsv_wf, resamples, metrics = custom_metrics, control = cv_control)
qda_hsv_cv <- fit_resamples(qda_hsv_wf, resamples, metrics = custom_metrics, control = cv_control)
```

```{r}
#| message: FALSE
#| warning: FALSE

# Fit final models on train_data for RGB
final_logreg_rgb_fit <- logreg_rgb_wf %>% fit(data = train_data)
final_lda_rgb_fit    <- lda_rgb_wf %>% fit(data = train_data)
final_qda_rgb_fit    <- qda_rgb_wf %>% fit(data = train_data)

# Fit final models on train_data for CIELab
final_logreg_lab_fit <- logreg_lab_wf %>% fit(data = train_data)
final_lda_lab_fit    <- lda_lab_wf %>% fit(data = train_data)
final_qda_lab_fit    <- qda_lab_wf %>% fit(data = train_data)

# Fit final models on train_data for HSV
final_logreg_hsv_fit <- logreg_hsv_wf %>% fit(data = train_data)
final_lda_hsv_fit    <- lda_hsv_wf %>% fit(data = train_data)
final_qda_hsv_fit    <- qda_hsv_wf %>% fit(data = train_data)
```

### Model performance before threshold selection
The performance metrics estimated using 10-fold cross-validation.
```{r cv metrics}
cv_metrics <- bind_rows(
    # RGB models
    collect_metrics(logreg_rgb_cv) %>% mutate(model = "Logistic Regression (RGB)"),
    collect_metrics(lda_rgb_cv) %>% mutate(model = "LDA (RGB)"),
    collect_metrics(qda_rgb_cv) %>% mutate(model = "QDA (RGB)"),
    
    # CIELab models
    collect_metrics(logreg_lab_cv) %>% mutate(model = "Logistic Regression (CIELab)"),
    collect_metrics(lda_lab_cv) %>% mutate(model = "LDA (CIELab)"),
    collect_metrics(qda_lab_cv) %>% mutate(model = "QDA (CIELab)"),
    
    # HSV models
    collect_metrics(logreg_hsv_cv) %>% mutate(model = "Logistic Regression (HSV)"),
    collect_metrics(lda_hsv_cv) %>% mutate(model = "LDA (HSV)"),
    collect_metrics(qda_hsv_cv) %>% mutate(model = "QDA (HSV)")
)

# Format and display metrics table
cv_metrics %>%
    select(model, .metric, mean) %>%
    pivot_wider(names_from = .metric, values_from = mean) %>%
    knitr::kable(
      caption = "Cross-validation performance metrics.", 
      digits = 3,
      col.names = c("Model", "Accuracy", "F-measure", "Precision", "ROC-AUC")
    ) %>%
    kableExtra::kable_styling(full_width = FALSE, position = "center")
```

Visualization of the same data
```{r cv-metrics-figure}
#| fig.cap: Cross-validation performance metrics
#| fig.width: 6
#| fig.height: 4
#| out.width: 75%
ggplot(cv_metrics, aes(x=mean, y=model, xmin=mean - std_err, xmax=mean + std_err)) +
    geom_point() +
    geom_linerange() +
    facet_wrap(~ .metric)
```

Cross-validation ROC curves
```{r cv-roc-curves-overlay}
#| fig.width: 16
#| fig.height: 5
#| fig.cap: Cross-validation ROC curves for each color space

# RGB Models
rgb_roc <- bind_rows(
    collect_predictions(logreg_rgb_cv) %>% mutate(model = "Logistic Regression (RGB)"),
    collect_predictions(lda_rgb_cv) %>% mutate(model = "LDA (RGB)"),
    collect_predictions(qda_rgb_cv) %>% mutate(model = "QDA (RGB)")
) %>%
    group_by(model) %>%
    roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
    autoplot() +
    ggtitle("ROC Curve - RGB Models") +
    theme_minimal()

# CIELab Models
lab_roc <- bind_rows(
    collect_predictions(logreg_lab_cv) %>% mutate(model = "Logistic Regression (CIELab)"),
    collect_predictions(lda_lab_cv) %>% mutate(model = "LDA (CIELab)"),
    collect_predictions(qda_lab_cv) %>% mutate(model = "QDA (CIELab)")
) %>%
    group_by(model) %>%
    roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
    autoplot() +
    ggtitle("ROC Curve - CIELab Models") +
    theme_minimal()

# HSV Models
hsv_roc <- bind_rows(
    collect_predictions(logreg_hsv_cv) %>% mutate(model = "Logistic Regression (HSV)"),
    collect_predictions(lda_hsv_cv) %>% mutate(model = "LDA (HSV)"),
    collect_predictions(qda_hsv_cv) %>% mutate(model = "QDA (HSV)")
) %>%
    group_by(model) %>%
    roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
    autoplot() +
    ggtitle("ROC Curve - HSV Models") +
    theme_minimal()

rgb_roc + lab_roc + hsv_roc
```

### Threshold selection/Optimization
It is clear that our outcome classes are heavily imbalanced, so we need to adjust the threshold to improve its predictive accuracy and precision.

Use package `probably` to explore the threshold. We define two functions to look at the effect of threshold selection on performance metrics and the associated confusion matrices:

```{r}
# Create metric set
class_metrics <- metric_set(accuracy, sens, f_meas)

# Compute metrics
compute_my_metrics <- function(data) {
  res_class <- class_metrics(data, truth = BT, estimate = .pred_class)
  res_prob  <- roc_auc(data, truth = BT, .pred_TRUE, event_level = "first")
  bind_rows(res_class, res_prob)
}

```

```{r}
calculate_metrics_cv <- function(cv_object, holdout, model_name, workflow, train_data) {
  # Use CV preds
  train_aug <- collect_predictions(cv_object)
  
  # Augment with holdout
  final_fit   <- workflow %>% fit(train_data)
  holdout_aug <- augment(final_fit, new_data = holdout)
  
  bind_rows(
    bind_cols(
      model   = model_name,
      dataset = "train",
      compute_my_metrics(train_aug)
    ),
    bind_cols(
      model   = model_name,
      dataset = "holdout",
      compute_my_metrics(holdout_aug)
    )
  )
}
```

```{r metrics table}
#| warning: FALSE
#| message: FALSE
all_metrics <- bind_rows(
    # RGB Models
    calculate_metrics_cv(logreg_rgb_cv, holdout_data, "Logistic Regression (RGB)", logreg_rgb_wf, train_data),
    calculate_metrics_cv(lda_rgb_cv, holdout_data, "LDA (RGB)", lda_rgb_wf, train_data),
    calculate_metrics_cv(qda_rgb_cv, holdout_data, "QDA (RGB)", qda_rgb_wf, train_data),
    
    # CIELab Models
    calculate_metrics_cv(logreg_lab_cv, holdout_data, "Logistic Regression (CIELab)", logreg_lab_wf, train_data),
    calculate_metrics_cv(lda_lab_cv, holdout_data, "LDA (CIELab)", lda_lab_wf, train_data),
    calculate_metrics_cv(qda_lab_cv, holdout_data, "QDA (CIELab)", qda_lab_wf, train_data),
    
    # HSV Models
    calculate_metrics_cv(logreg_hsv_cv, holdout_data, "Logistic Regression (HSV)", logreg_hsv_wf, train_data),
    calculate_metrics_cv(lda_hsv_cv, holdout_data, "LDA (HSV)", lda_hsv_wf, train_data),
    calculate_metrics_cv(qda_hsv_cv, holdout_data, "QDA (HSV)", qda_hsv_wf, train_data)
) %>% arrange(dataset)
```

```{r metrics table 2}
all_metrics %>%
        pivot_wider(names_from=.metric, values_from=.estimate) %>%
        select(-.estimator) %>%
        knitr::kable(
          caption= "Metrics for the classification models.", 
          digits=3) %>%
        kableExtra::kable_styling(full_width=FALSE)
```


```{r threshold graphs 1}
threshold_graph <- function(model_cv, model_name) {
    performance <- probably::threshold_perf(collect_predictions(model_cv), BT, .pred_TRUE,
        thresholds=seq(0.01, 0.99, 0.01), event_level="first",
        metrics=metric_set(f_meas, accuracy, sens))
    max_metrics <- performance %>%
        drop_na() %>%
        group_by(.metric) %>%
        filter(.estimate == max(.estimate))
    g <- ggplot(performance, aes(x=.threshold, y=.estimate, color=.metric)) +
        geom_line() +
        geom_point(data=max_metrics, color="black") +
        labs(title=model_name, x="Threshold", y="Metric value") +
        coord_cartesian(ylim=c(0, 1))
    thresholds <- max_metrics %>%
        select(.metric, .threshold) %>%
        deframe()
    return(list(graph=g, thresholds=thresholds))
}

visualize_conf_mat <- function(model_cv, thresholds, metric) {
    threshold <- thresholds[metric]
    cm <- collect_predictions(model_cv) %>%
        mutate(
            .pred_class = make_two_class_pred(.pred_TRUE, c("TRUE", "FALSE"), threshold=threshold),
        ) %>%
        conf_mat(truth=BT, estimate=.pred_class)
    autoplot(cm, type="heatmap") +
        labs(title=sprintf("Threshold %.2f (%s)", threshold, metric))
}

overview_model <- function(model_cv, model_name) {
    tg <- threshold_graph(model_cv, model_name)
    g1 <- visualize_conf_mat(model_cv, tg$thresholds, "accuracy")
    g2 <- visualize_conf_mat(model_cv, tg$thresholds, "f_meas")
    g3 <- visualize_conf_mat(model_cv, tg$thresholds, "sens")
    tg$graph + (g1 / g2 / g3)
}
```

Notes:

- `f_meas` cannot be calculated for high threshold values. In this case, the function `threshold_perf` returns `NA` for the F-measure. We filter out these values using `drop_na()`.

```{r print threshold graphs rgb}
#| fig.width: 8
#| fig.height: 12
#| out.width: 80%
#| fig.cap: Metrics as a function of model performance
#| warning: FALSE
g1 <- overview_model(logreg_rgb_cv, "Logistic regression")
g2 <- overview_model(lda_rgb_cv, "LDA")
g3 <- overview_model(qda_rgb_cv, "QDA")

g1 / g2 / g3
```

```{r print threshold graphs cielab}
#| fig.width: 8
#| fig.height: 12
#| out.width: 80%
#| fig.cap: Metrics as a function of model performance
#| warning: FALSE
g1 <- overview_model(logreg_lab_cv, "Logistic regression")
g2 <- overview_model(lda_lab_cv, "LDA")
g3 <- overview_model(qda_lab_cv, "QDA")

g1 / g2 / g3
```

```{r print threshold graphs hsv}
#| fig.width: 8
#| fig.height: 12
#| out.width: 80%
#| fig.cap: Metrics as a function of model performance
#| warning: FALSE
g1 <- overview_model(logreg_hsv_cv, "Logistic regression")
g2 <- overview_model(lda_hsv_cv, "LDA")
g3 <- overview_model(qda_hsv_cv, "QDA")

g1 / g2 / g3
```

```{r threshold graphs f_meas}
# Tweak f measure
f_meas_adj2 <- metric_tweak("f_meas_adj2", f_meas, beta = 2)
f_meas_adj3 <- metric_tweak("f_meas_adj3", f_meas, beta = 3)

threshold_graph <- function(model_cv, model_name) {
    performance <- probably::threshold_perf(collect_predictions(model_cv), BT, .pred_TRUE,
        thresholds=seq(0.01, 0.99, 0.01), event_level="first",
        metrics=metric_set(f_meas, f_meas_adj2, f_meas_adj3))
    max_metrics <- performance %>%
        drop_na() %>%
        group_by(.metric) %>%
        filter(.estimate == max(.estimate))
    g <- ggplot(performance, aes(x=.threshold, y=.estimate, color=.metric)) +
        geom_line() +
        geom_point(data=max_metrics, color="black") +
        labs(title=model_name, x="Threshold", y="Metric value") +
        coord_cartesian(ylim=c(0, 1))
    thresholds <- max_metrics %>%
        select(.metric, .threshold) %>%
        deframe()
    return(list(graph=g, thresholds=thresholds))
}

visualize_conf_mat <- function(model_cv, thresholds, metric) {
    threshold <- thresholds[metric]
    cm <- collect_predictions(model_cv) %>%
        mutate(
            .pred_class = make_two_class_pred(.pred_TRUE, c("TRUE", "FALSE"), threshold=threshold),
        ) %>%
        conf_mat(truth=BT, estimate=.pred_class)
    autoplot(cm, type="heatmap") +
        labs(title=sprintf("Threshold %.2f (%s)", threshold, metric))
}

overview_model <- function(model_cv, model_name) {
    tg <- threshold_graph(model_cv, model_name)
    g1 <- visualize_conf_mat(model_cv, tg$thresholds, "f_meas")
    g2 <- visualize_conf_mat(model_cv, tg$thresholds, "f_meas_adj2")
    g3 <- visualize_conf_mat(model_cv, tg$thresholds, "f_meas_adj3")
    tg$graph + (g1 / g2 / g3)
}
```

```{r print threshold graphs f_meas rgb}
#| fig.width: 8
#| fig.height: 12
#| out.width: 80%
#| fig.cap: Metrics as a function of model performance
#| warning: FALSE
g1 <- overview_model(logreg_rgb_cv, "Logistic regression")
g2 <- overview_model(lda_rgb_cv, "LDA")
g3 <- overview_model(qda_rgb_cv, "QDA")

g1 / g2 / g3
```

```{r print threshold graphs f_meas lab}
#| fig.width: 8
#| fig.height: 12
#| out.width: 80%
#| fig.cap: Metrics as a function of model performance
#| warning: FALSE
g1 <- overview_model(logreg_lab_cv, "Logistic regression")
g2 <- overview_model(lda_lab_cv, "LDA")
g3 <- overview_model(qda_lab_cv, "QDA")

g1 / g2 / g3
```

```{r print threshold graphs f_meas hsv}
#| fig.width: 8
#| fig.height: 12
#| out.width: 80%
#| fig.cap: Metrics as a function of model performance
#| warning: FALSE
g1 <- overview_model(logreg_hsv_cv, "Logistic regression")
g2 <- overview_model(lda_hsv_cv, "LDA")
g3 <- overview_model(qda_hsv_cv, "QDA")

g1 / g2 / g3
```

Next, I compared the ROC curves for the logistic regression between the cross-validation predictions and the predictions on the full training data set to see if the logistic regression was overfitting the training data.

```{r}
#| fig.width: 12
#| fig.height: 9
#| fig.cap: ROC curve comparison between cross-validation and full data set predictions
#| warning: FALSE

# Function to generate ROC overlay plots
get_roc_overlay_autoplot <- function(cv_object, wf, data, model_name) {
  # Fit the full model
  full_fit <- wf %>% fit(data)
  
  # Get CV preds
  cv_preds <- collect_predictions(cv_object) %>% 
    mutate(source = "CV")
  
  # Get preds for full model
  full_preds <- augment(full_fit, new_data = data) %>% 
    mutate(source = "Full")
  
  # Compute ROC
  roc_data <- bind_rows(cv_preds, full_preds) %>%
    group_by(source) %>%
    roc_curve(truth = BT, .pred_TRUE, event_level = "first")
  
  # Use autoplot
  autoplot(roc_data) +
    ggtitle(paste(model_name)) +
    theme_minimal()
}

# Generate ROC overlay plots for each model

## RGB Models
p_logreg_rgb <- get_roc_overlay_autoplot(logreg_rgb_cv, logreg_rgb_wf, train_data, "Logistic Regression (RGB)")
p_lda_rgb    <- get_roc_overlay_autoplot(lda_rgb_cv, lda_rgb_wf, train_data, "LDA (RGB)")
p_qda_rgb    <- get_roc_overlay_autoplot(qda_rgb_cv, qda_rgb_wf, train_data, "QDA (RGB)")

## CIELab Models
p_logreg_lab <- get_roc_overlay_autoplot(logreg_lab_cv, logreg_lab_wf, train_data, "Logistic Regression (CIELab)")
p_lda_lab    <- get_roc_overlay_autoplot(lda_lab_cv, lda_lab_wf, train_data, "LDA (CIELab)")
p_qda_lab    <- get_roc_overlay_autoplot(qda_lab_cv, qda_lab_wf, train_data, "QDA (CIELab)")

## HSV Models
p_logreg_hsv <- get_roc_overlay_autoplot(logreg_hsv_cv, logreg_hsv_wf, train_data, "Logistic Regression (HSV)")
p_lda_hsv    <- get_roc_overlay_autoplot(lda_hsv_cv, lda_hsv_wf, train_data, "LDA (HSV)")
p_qda_hsv    <- get_roc_overlay_autoplot(qda_hsv_cv, qda_hsv_wf, train_data, "QDA (HSV)")

# Combine plots
combined_roc <- (p_logreg_rgb + p_lda_rgb + p_qda_rgb) / 
                (p_logreg_lab + p_lda_lab + p_qda_lab) / 
                (p_logreg_hsv + p_lda_hsv + p_qda_hsv) +
                plot_annotation(title = "Overlay of ROC Curves (CV vs. Full Data Predictions)")

combined_roc
```

As we can see that the ROC curve for the cross-validation predictions is almost identical to the ROC curve for the predictions on the full training set, indicating that the logistic regression model is not overfitting the training data.

```{r optimal thresholds}
# Find optimal thresholds
threshold_scan <- function(model, data, model_name) {
  threshold_data <- model %>%
    augment(data) %>%
    probably::threshold_perf(
      truth = BT,
      estimate = .pred_TRUE,
      thresholds = seq(0.01, 0.99, 0.01),
      event_level = "first",
      metrics = metric_set(f_meas)
    )
  opt_threshold <- threshold_data %>%
    drop_na() %>%
    arrange(desc(.estimate)) %>%
    slice(1)
  list(
    threshold = opt_threshold$.threshold,
    threshold_data = threshold_data,
    opt_threshold = opt_threshold,
    model_name = model_name
  )
}
```

```{r}
# Threshold scanning
logreg_rgb_result <- threshold_scan(final_logreg_rgb_fit, holdout_data, "Logistic Regression (RGB)")
lda_rgb_result    <- threshold_scan(final_lda_rgb_fit, holdout_data, "LDA (RGB)")
qda_rgb_result    <- threshold_scan(final_qda_rgb_fit, holdout_data, "QDA (RGB)")

logreg_lab_result <- threshold_scan(final_logreg_lab_fit, holdout_data, "Logistic Regression (CIELab)")
lda_lab_result    <- threshold_scan(final_lda_lab_fit, holdout_data, "LDA (CIELab)")
qda_lab_result    <- threshold_scan(final_qda_lab_fit, holdout_data, "QDA (CIELab)")

logreg_hsv_result <- threshold_scan(final_logreg_hsv_fit, holdout_data, "Logistic Regression (HSV)")
lda_hsv_result    <- threshold_scan(final_lda_hsv_fit, holdout_data, "LDA (HSV)")
qda_hsv_result    <- threshold_scan(final_qda_hsv_fit, holdout_data, "QDA (HSV)")
```

```{r}
# Optimal thresholds
logreg_rgb_holdout_threshold <- logreg_rgb_result$threshold
lda_rgb_holdout_threshold    <- lda_rgb_result$threshold
qda_rgb_holdout_threshold    <- qda_rgb_result$threshold

logreg_lab_holdout_threshold <- logreg_lab_result$threshold
lda_lab_holdout_threshold    <- lda_lab_result$threshold
qda_lab_holdout_threshold    <- qda_lab_result$threshold

logreg_hsv_holdout_threshold <- logreg_hsv_result$threshold
lda_hsv_holdout_threshold    <- lda_hsv_result$threshold
qda_hsv_holdout_threshold    <- qda_hsv_result$threshold
```

```{r}
threshold_scan_cv <- function(cv_obj, model_name) {
  threshold_data <- cv_obj %>%
    collect_predictions() %>%
    probably::threshold_perf(
      truth = BT,
      estimate = .pred_TRUE,
      thresholds = seq(0.05, 0.95, 0.01),
      event_level = "first",
      metrics = metric_set(f_meas)
    )
  opt_threshold <- threshold_data %>%
    drop_na() %>%
    arrange(desc(.estimate)) %>%
    slice(1)
  list(
    threshold = opt_threshold$.threshold
  )
}

# Compute thresholds

## RGB Models
logreg_rgb_train_result <- threshold_scan_cv(logreg_rgb_cv, "Logistic Regression (RGB)")
lda_rgb_train_result    <- threshold_scan_cv(lda_rgb_cv, "LDA (RGB)")
qda_rgb_train_result    <- threshold_scan_cv(qda_rgb_cv, "QDA (RGB)")

## CIELab Models
logreg_lab_train_result <- threshold_scan_cv(logreg_lab_cv, "Logistic Regression (CIELab)")
lda_lab_train_result    <- threshold_scan_cv(lda_lab_cv, "LDA (CIELab)")
qda_lab_train_result    <- threshold_scan_cv(qda_lab_cv, "QDA (CIELab)")

## HSV Models
logreg_hsv_train_result <- threshold_scan_cv(logreg_hsv_cv, "Logistic Regression (HSV)")
lda_hsv_train_result    <- threshold_scan_cv(lda_hsv_cv, "LDA (HSV)")
qda_hsv_train_result    <- threshold_scan_cv(qda_hsv_cv, "QDA (HSV)")

# Extract optimal thresholds

## RGB
logreg_rgb_train_threshold <- logreg_rgb_train_result$threshold
lda_rgb_train_threshold    <- lda_rgb_train_result$threshold
qda_rgb_train_threshold    <- qda_rgb_train_result$threshold

## CIELab
logreg_lab_train_threshold <- logreg_lab_train_result$threshold
lda_lab_train_threshold    <- lda_lab_train_result$threshold
qda_lab_train_threshold    <- qda_lab_train_result$threshold

## HSV
logreg_hsv_train_threshold <- logreg_hsv_train_result$threshold
lda_hsv_train_threshold    <- lda_hsv_train_result$threshold
qda_hsv_train_threshold    <- qda_hsv_train_result$threshold
```

```{r combine threshold graphs}
#| fig.width: 12
#| fig.height: 9
#| fig.cap: F-Measure by threshold for each model and color space
#| warning: FALSE

# Function to plot threshold performance
plot_threshold <- function(result) {
  ggplot(result$threshold_data, aes(x = .threshold, y = .estimate)) +
    geom_line() +
    geom_point(data = result$opt_threshold, color = "red", size = 2) +
    labs(title = result$model_name, x = "Threshold", y = "F-Measure") +
    coord_cartesian(ylim = c(0, 1))
}

## RGB
g_logreg_rgb <- plot_threshold(logreg_rgb_result)
g_lda_rgb    <- plot_threshold(lda_rgb_result)
g_qda_rgb    <- plot_threshold(qda_rgb_result)

## CIELab
g_logreg_lab <- plot_threshold(logreg_lab_result)
g_lda_lab    <- plot_threshold(lda_lab_result)
g_qda_lab    <- plot_threshold(qda_lab_result)

## HSV
g_logreg_hsv <- plot_threshold(logreg_hsv_result)
g_lda_hsv    <- plot_threshold(lda_hsv_result)
g_qda_hsv    <- plot_threshold(qda_hsv_result)

# Combine plots
combined_thresholds <- (g_logreg_rgb + g_lda_rgb + g_qda_rgb) /
                       (g_logreg_lab + g_lda_lab + g_qda_lab) /
                       (g_logreg_hsv + g_lda_hsv + g_qda_hsv) +
                       plot_annotation(title = "Threshold Performance (F-Measure) Across Color Spaces")

combined_thresholds
```

```{r model evaluation}
predict_at_threshold <- function(model, data, threshold) {
  model %>%
    augment(data) %>%
    mutate(.pred_class = make_two_class_pred(.pred_TRUE,
                     c("TRUE", "FALSE"),
                     threshold = threshold))
}

calculate_metrics_at_threshold <- function(model, train, holdout, model_name, train_threshold, holdout_threshold) {
  bind_rows(
    # Metrics for the training set
    bind_cols(
      model = model_name,
      dataset = "train",
      threshold = train_threshold,
      metrics(predict_at_threshold(model, train, train_threshold), truth = BT, estimate = .pred_class)
    ),
    bind_cols(
      model = model_name,
      dataset = "train",
      threshold = train_threshold,
      roc_auc(model %>% augment(train), BT, .pred_TRUE, event_level = "first")
    ),
    bind_cols(
      model = model_name,
      dataset = "train",
      threshold = train_threshold,
      f_meas(predict_at_threshold(model, train, train_threshold), truth = BT, estimate = .pred_class)
    ),
    bind_cols(
      model = model_name,
      dataset = "train",
      threshold = train_threshold,
      sens(predict_at_threshold(model, train, train_threshold), truth = BT, estimate = .pred_class)
    ),
    # Metrics for the holdout set
    bind_cols(
      model = model_name,
      dataset = "holdout",
      threshold = holdout_threshold,
      metrics(predict_at_threshold(model, holdout, holdout_threshold), truth = BT, estimate = .pred_class)
    ),
    bind_cols(
      model = model_name,
      dataset = "holdout",
      threshold = holdout_threshold,
      roc_auc(model %>% augment(holdout), BT, .pred_TRUE, event_level = "first")
    ),
    bind_cols(
      model = model_name,
      dataset = "holdout",
      threshold = holdout_threshold,
      f_meas(predict_at_threshold(model, holdout, holdout_threshold), truth = BT, estimate = .pred_class)
    ),
    bind_cols(
      model = model_name,
      dataset = "holdout",
      threshold = holdout_threshold,
      sens(predict_at_threshold(model, holdout, holdout_threshold), truth = BT, estimate = .pred_class)
    )
  )
}

metrics_at_threshold <- bind_rows(
    # RGB Models
    calculate_metrics_at_threshold(final_logreg_rgb_fit, train_data, holdout_data, 
                                   "Logistic Regression (RGB)", logreg_rgb_train_threshold, logreg_rgb_holdout_threshold),
    calculate_metrics_at_threshold(final_lda_rgb_fit, train_data, holdout_data, 
                                   "LDA (RGB)", lda_rgb_train_threshold, lda_rgb_holdout_threshold),
    calculate_metrics_at_threshold(final_qda_rgb_fit, train_data, holdout_data, 
                                   "QDA (RGB)", qda_rgb_train_threshold, qda_rgb_holdout_threshold),

    # CIELab Models
    calculate_metrics_at_threshold(final_logreg_lab_fit, train_data, holdout_data, 
                                   "Logistic Regression (CIELab)", logreg_lab_train_threshold, logreg_lab_holdout_threshold),
    calculate_metrics_at_threshold(final_lda_lab_fit, train_data, holdout_data, 
                                   "LDA (CIELab)", lda_lab_train_threshold, lda_lab_holdout_threshold),
    calculate_metrics_at_threshold(final_qda_lab_fit, train_data, holdout_data, 
                                   "QDA (CIELab)", qda_lab_train_threshold, qda_lab_holdout_threshold),

    # HSV Models
    calculate_metrics_at_threshold(final_logreg_hsv_fit, train_data, holdout_data, 
                                   "Logistic Regression (HSV)", logreg_hsv_train_threshold, logreg_hsv_holdout_threshold),
    calculate_metrics_at_threshold(final_lda_hsv_fit, train_data, holdout_data, 
                                   "LDA (HSV)", lda_hsv_train_threshold, lda_hsv_holdout_threshold),
    calculate_metrics_at_threshold(final_qda_hsv_fit, train_data, holdout_data, 
                                   "QDA (HSV)", qda_hsv_train_threshold, qda_hsv_holdout_threshold)
) %>% arrange(dataset)
```

```{r model eval table}
metrics_at_threshold %>%
        pivot_wider(names_from=.metric, values_from=.estimate) %>%
        select(-.estimator) %>%
        knitr::kable(
          caption= "Performance metrics for models at ideal threshold.", 
          digits=3) %>%
        kableExtra::kable_styling(full_width=FALSE)
```

```{r}
# Function to generate confusion matrix plots
visualize_conf_mat_holdout <- function(model, data, threshold, metric_label) {
  cm <- model %>%
    augment(data) %>%
    mutate(.pred_class = make_two_class_pred(.pred_TRUE, c("TRUE", "FALSE"), threshold = threshold)) %>%
    conf_mat(truth = BT, estimate = .pred_class)
  
  autoplot(cm, type = "heatmap") +
    labs(title = sprintf("Threshold %.2f (%s)", threshold, metric_label))
}

# Function to generate overview confusion matrix plots
overview_model_holdout <- function(model, model_name, threshold) {
  cm_plot <- visualize_conf_mat_holdout(model, holdout_data, threshold, "Holdout")
  cm_plot + labs(title = sprintf("%s \n (Threshold = %.2f)", model_name, threshold))
}

## RGB Models
logreg_cm_rgb <- overview_model_holdout(final_logreg_rgb_fit, "Logistic Regression (RGB)", logreg_rgb_holdout_threshold)
lda_cm_rgb    <- overview_model_holdout(final_lda_rgb_fit, "LDA (RGB)", lda_rgb_holdout_threshold)
qda_cm_rgb    <- overview_model_holdout(final_qda_rgb_fit, "QDA (RGB)", qda_rgb_holdout_threshold)

## CIELab Models
logreg_cm_lab <- overview_model_holdout(final_logreg_lab_fit, "Logistic Regression (CIELab)", logreg_lab_holdout_threshold)
lda_cm_lab    <- overview_model_holdout(final_lda_lab_fit, "LDA (CIELab)", lda_lab_holdout_threshold)
qda_cm_lab    <- overview_model_holdout(final_qda_lab_fit, "QDA (CIELab)", qda_lab_holdout_threshold)

## HSV Models
logreg_cm_hsv <- overview_model_holdout(final_logreg_hsv_fit, "Logistic Regression (HSV)", logreg_hsv_holdout_threshold)
lda_cm_hsv    <- overview_model_holdout(final_lda_hsv_fit, "LDA (HSV)", lda_hsv_holdout_threshold)
qda_cm_hsv    <- overview_model_holdout(final_qda_hsv_fit, "QDA (HSV)", qda_hsv_holdout_threshold)
```

```{r}
#| fig.width: 12
#| fig.height: 9
#| fig.cap: Holdout Set Confusion Matrices by Color Space
#| warning: FALSE
# Combine plots into a 3-row
combined_cm <- (logreg_cm_rgb + lda_cm_rgb + qda_cm_rgb) /
               (logreg_cm_lab + lda_cm_lab + qda_cm_lab) /
               (logreg_cm_hsv + lda_cm_hsv + qda_cm_hsv) +
               plot_annotation(title = "Holdout Set Confusion Matrices Across Color Spaces")

# Display the combined plot
combined_cm
```

```{r optimal thresholds}
# Find optimal thresholds
threshold_fine_scan <- function(model, data, model_name) {
  threshold_data <- model %>%
    augment(data) %>%
    probably::threshold_perf(
      truth = BT,
      estimate = .pred_TRUE,
      thresholds = seq(0.99, 1, 0.001),
      event_level = "first",
      metrics = metric_set(f_meas)
    )
  opt_threshold <- threshold_data %>%
    drop_na() %>%
    arrange(desc(.estimate)) %>%
    slice(1)
  list(
    threshold = opt_threshold$.threshold,
    threshold_data = threshold_data,
    opt_threshold = opt_threshold,
    model_name = model_name
  )
}
```

```{r}
#| warning: FALSE
#| message: FALSE
# Threshold scanning
logreg_rgb_fine_result <- threshold_fine_scan(final_logreg_rgb_fit, holdout_data, "Logistic Regression (RGB)")

logreg_lab_fine_result <- threshold_fine_scan(final_logreg_lab_fit, holdout_data, "Logistic Regression (CIELab)")
```

```{r}
# Optimal thresholds
logreg_rgb_fine_holdout_threshold <- logreg_rgb_fine_result$threshold

logreg_lab_fine_holdout_threshold <- logreg_lab_fine_result$threshold
```

```{r}
metrics_at_threshold <- bind_rows(
    # RGB Models
    calculate_metrics_at_threshold(final_logreg_rgb_fit, train_data, holdout_data, 
                                   "Logistic Regression (RGB)", logreg_rgb_train_threshold, logreg_rgb_fine_holdout_threshold),

    # CIELab Models
    calculate_metrics_at_threshold(final_logreg_lab_fit, train_data, holdout_data, 
                                   "Logistic Regression (CIELab)", logreg_lab_train_threshold, logreg_lab_fine_holdout_threshold)
) %>% arrange(dataset)
```

```{r model eval table fine}
metrics_at_threshold %>%
        pivot_wider(names_from=.metric, values_from=.estimate) %>%
        select(-.estimator) %>%
        knitr::kable(
          caption= "Performance metrics for models at ideal threshold.", 
          digits=3) %>%
        kableExtra::kable_styling(full_width=FALSE)
```

```{r}
stopCluster(cl)
registerDoSEQ()
```
